ccopy_reg
_reconstructor
p0
(cdigits.model.images.generic.job
GenericImageModelJob
p1
c__builtin__
object
p2
Ntp3
Rp4
(dp5
S'username'
p6
Vpedrodiamel
p7
sS'_notes'
p8
NsS'tasks'
p9
(lp10
g0
(cdigits.model.tasks.caffe_train
CaffeTrainTask
p11
g2
Ntp12
Rp13
(dp14
S'shuffle'
p15
I01
sS'snapshot_interval'
p16
F1.0
sS'train_outputs'
p17
ccollections
OrderedDict
p18
((lp19
(lp20
S'epoch'
p21
ag0
(cdigits.model.tasks.train
NetworkOutput
p22
c__builtin__
tuple
p23
(S'Epoch'
p24
(lp25
I0
aF0.1111111111111111
aF0.2222222222222222
aF0.3333333333333333
aF0.4444444444444444
aF0.5555555555555556
aF0.6666666666666666
aF0.7777777777777778
aF0.8888888888888888
aF1.0
aF1.1111111111111112
aF1.2222222222222223
aF1.3333333333333333
aF1.4444444444444444
aF1.5555555555555556
aF1.6666666666666667
aF1.7777777777777777
aF1.8888888888888888
aF2.0
aF2.111111111111111
aF2.2222222222222223
aF2.3333333333333335
aF2.4444444444444446
aF2.5555555555555554
aF2.6666666666666665
aF2.7777777777777777
aF2.888888888888889
aF3.0
aF3.111111111111111
aF3.2222222222222223
aF3.3333333333333335
aF3.4444444444444446
aF3.5555555555555554
aF3.6666666666666665
aF3.7777777777777777
aF3.888888888888889
aF4.0
aF4.111111111111111
aF4.222222222222222
aF4.333333333333333
aF4.444444444444445
aF4.555555555555555
aF4.666666666666667
aF4.777777777777778
aF4.888888888888889
aF5.0
aF5.111111111111111
aF5.222222222222222
aF5.333333333333333
aF5.444444444444445
aF5.555555555555555
aF5.666666666666667
aF5.777777777777778
aF5.888888888888889
aF6.0
aF6.111111111111111
aF6.222222222222222
aF6.333333333333333
aF6.444444444444445
aF6.555555555555555
aF6.666666666666667
aF6.777777777777778
aF6.888888888888889
aF7.0
aF7.111111111111111
aF7.222222222222222
aF7.333333333333333
aF7.444444444444445
aF7.555555555555555
aF7.666666666666667
aF7.777777777777778
aF7.888888888888889
aF8.0
aF8.11111111111111
aF8.222222222222221
aF8.333333333333334
aF8.444444444444445
aF8.555555555555555
aF8.666666666666666
aF8.777777777777779
aF8.88888888888889
aF9.0
aF9.11111111111111
aF9.222222222222221
aF9.333333333333334
aF9.444444444444445
aF9.555555555555555
aF9.666666666666666
aF9.777777777777779
aF9.88888888888889
aF10.0
aF10.11111111111111
aF10.222222222222221
aF10.333333333333334
aF10.444444444444445
aF10.555555555555555
aF10.666666666666666
aF10.777777777777779
aF10.88888888888889
aF11.0
aF11.11111111111111
aF11.222222222222221
aF11.333333333333334
aF11.444444444444445
aF11.555555555555555
aF11.666666666666666
aF11.777777777777779
aF11.88888888888889
aF12.0
aF12.11111111111111
aF12.222222222222221
aF12.333333333333334
aF12.444444444444445
aF12.555555555555555
aF12.666666666666666
aF12.777777777777779
aF12.88888888888889
aF13.0
aF13.11111111111111
aF13.222222222222221
aF13.333333333333334
aF13.444444444444445
aF13.555555555555555
aF13.666666666666666
aF13.777777777777779
aF13.88888888888889
aF14.0
aF14.11111111111111
aF14.222222222222221
aF14.333333333333334
aF14.444444444444445
aF14.555555555555555
aF14.666666666666666
aF14.777777777777779
aF14.88888888888889
aF15.0
aF15.11111111111111
aF15.222222222222221
aF15.333333333333334
aF15.444444444444445
aF15.555555555555555
aF15.666666666666666
aF15.777777777777779
aF15.88888888888889
aF16.0
aF16.11111111111111
aF16.22222222222222
aF16.333333333333332
aF16.444444444444443
aF16.555555555555557
aF16.666666666666668
aF16.77777777777778
aF16.88888888888889
aF17.0
aF17.11111111111111
aF17.22222222222222
aF17.333333333333332
aF17.444444444444443
aF17.555555555555557
aF17.666666666666668
aF17.77777777777778
aF17.88888888888889
aF18.0
aF18.11111111111111
aF18.22222222222222
aF18.333333333333332
aF18.444444444444443
aF18.555555555555557
aF18.666666666666668
aF18.77777777777778
aF18.88888888888889
aF19.0
aF19.11111111111111
aF19.22222222222222
aF19.333333333333332
aF19.444444444444443
aF19.555555555555557
aF19.666666666666668
aF19.77777777777778
aF19.88888888888889
aF20.0
aF20.11111111111111
aF20.22222222222222
aF20.333333333333332
aF20.444444444444443
aF20.555555555555557
aF20.666666666666668
aF20.77777777777778
aF20.88888888888889
aF21.0
aF21.11111111111111
aF21.22222222222222
aF21.333333333333332
aF21.444444444444443
aF21.555555555555557
aF21.666666666666668
aF21.77777777777778
aF21.88888888888889
aF22.0
aF22.11111111111111
aF22.22222222222222
aF22.333333333333332
aF22.444444444444443
aF22.555555555555557
aF22.666666666666668
aF22.77777777777778
aF22.88888888888889
aF23.0
aF23.11111111111111
aF23.22222222222222
aF23.333333333333332
aF23.444444444444443
aF23.555555555555557
aF23.666666666666668
aF23.77777777777778
aF23.88888888888889
aF24.0
aF24.11111111111111
aF24.22222222222222
aF24.333333333333332
aF24.444444444444443
aF24.555555555555557
aF24.666666666666668
aF24.77777777777778
aF24.88888888888889
aF25.0
aF25.11111111111111
aF25.22222222222222
aF25.333333333333332
aF25.444444444444443
aF25.555555555555557
aF25.666666666666668
aF25.77777777777778
aF25.88888888888889
aF26.0
aF26.11111111111111
aF26.22222222222222
aF26.333333333333332
aF26.444444444444443
aF26.555555555555557
aF26.666666666666668
aF26.77777777777778
aF26.88888888888889
aF27.0
aF27.11111111111111
aF27.22222222222222
aF27.333333333333332
aF27.444444444444443
aF27.555555555555557
aF27.666666666666668
aF27.77777777777778
aF27.88888888888889
aF28.0
aF28.11111111111111
aF28.22222222222222
aF28.333333333333332
aF28.444444444444443
aF28.555555555555557
aF28.666666666666668
aF28.77777777777778
aF28.88888888888889
aF29.0
aF29.11111111111111
aF29.22222222222222
aF29.333333333333332
aF29.444444444444443
aF29.555555555555557
aF29.666666666666668
aF29.77777777777778
aF29.88888888888889
aF30.0
aF30.11111111111111
aF30.22222222222222
aF30.333333333333332
aF30.444444444444443
aF30.555555555555557
aF30.666666666666668
aF30.77777777777778
aF30.88888888888889
aF31.0
aF31.11111111111111
aF31.22222222222222
aF31.333333333333332
aF31.444444444444443
aF31.555555555555557
aF31.666666666666668
aF31.77777777777778
aF31.88888888888889
aF32.0
aF32.111111111111114
aF32.22222222222222
aF32.333333333333336
aF32.44444444444444
aF32.55555555555556
aF32.666666666666664
aF32.77777777777778
aF32.888888888888886
aF33.0
aF33.111111111111114
aF33.22222222222222
aF33.333333333333336
aF33.44444444444444
aF33.55555555555556
aF33.666666666666664
aF33.77777777777778
aF33.888888888888886
aF34.0
aF34.111111111111114
aF34.22222222222222
aF34.333333333333336
aF34.44444444444444
aF34.55555555555556
aF34.666666666666664
aF34.77777777777778
aF34.888888888888886
aF35.0
aF35.111111111111114
aF35.22222222222222
aF35.333333333333336
aF35.44444444444444
aF35.55555555555556
aF35.666666666666664
aF35.77777777777778
aF35.888888888888886
aF36.0
aF36.111111111111114
aF36.22222222222222
aF36.333333333333336
aF36.44444444444444
aF36.55555555555556
aF36.666666666666664
aF36.77777777777778
aF36.888888888888886
aF37.0
aF37.111111111111114
aF37.22222222222222
aF37.333333333333336
aF37.44444444444444
aF37.55555555555556
aF37.666666666666664
aF37.77777777777778
aF37.888888888888886
aF38.0
aF38.111111111111114
aF38.22222222222222
aF38.333333333333336
aF38.44444444444444
aF38.55555555555556
aF38.666666666666664
aF38.77777777777778
aF38.888888888888886
aF39.0
aF39.111111111111114
aF39.22222222222222
aF39.333333333333336
aF39.44444444444444
aF39.55555555555556
aF39.666666666666664
aF39.77777777777778
aF39.888888888888886
aF40.0
aF40.111111111111114
aF40.22222222222222
aF40.333333333333336
aF40.44444444444444
aF40.55555555555556
aF40.666666666666664
aF40.77777777777778
aF40.888888888888886
aF41.0
aF41.111111111111114
aF41.22222222222222
aF41.333333333333336
aF41.44444444444444
aF41.55555555555556
aF41.666666666666664
aF41.77777777777778
aF41.888888888888886
aF42.0
aF42.111111111111114
aF42.22222222222222
aF42.333333333333336
aF42.44444444444444
aF42.55555555555556
aF42.666666666666664
aF42.77777777777778
aF42.888888888888886
aF43.0
aF43.111111111111114
aF43.22222222222222
aF43.333333333333336
aF43.44444444444444
aF43.55555555555556
aF43.666666666666664
aF43.77777777777778
aF43.888888888888886
aF44.0
aF44.111111111111114
aF44.22222222222222
aF44.333333333333336
aF44.44444444444444
aF44.55555555555556
aF44.666666666666664
aF44.77777777777778
aF44.888888888888886
aF45.0
aF45.111111111111114
aF45.22222222222222
aF45.333333333333336
aF45.44444444444444
aF45.55555555555556
aF45.666666666666664
aF45.77777777777778
aF45.888888888888886
aF46.0
aF46.111111111111114
aF46.22222222222222
aF46.333333333333336
aF46.44444444444444
aF46.55555555555556
aF46.666666666666664
aF46.77777777777778
aF46.888888888888886
aF47.0
aF47.111111111111114
aF47.22222222222222
aF47.333333333333336
aF47.44444444444444
aF47.55555555555556
aF47.666666666666664
aF47.77777777777778
aF47.888888888888886
aF48.0
aF48.111111111111114
aF48.22222222222222
aF48.333333333333336
aF48.44444444444444
aF48.55555555555556
aF48.666666666666664
aF48.77777777777778
aF48.888888888888886
aF49.0
aF49.111111111111114
aF49.22222222222222
aF49.333333333333336
aF49.44444444444444
aF49.55555555555556
aF49.666666666666664
aF49.77777777777778
aF49.888888888888886
aF50.0
aF50.111111111111114
aF50.22222222222222
aF50.333333333333336
aF50.44444444444444
aF50.55555555555556
aF50.666666666666664
aF50.77777777777778
aF50.888888888888886
aF51.0
aF51.111111111111114
aF51.22222222222222
aF51.333333333333336
aF51.44444444444444
aF51.55555555555556
aF51.666666666666664
aF51.77777777777778
aF51.888888888888886
aF52.0
aF52.111111111111114
aF52.22222222222222
aF52.333333333333336
aF52.44444444444444
aF52.55555555555556
aF52.666666666666664
aF52.77777777777778
aF52.888888888888886
aF53.0
aF53.111111111111114
aF53.22222222222222
aF53.333333333333336
aF53.44444444444444
aF53.55555555555556
aF53.666666666666664
aF53.77777777777778
aF53.888888888888886
aF54.0
aF54.111111111111114
aF54.22222222222222
aF54.333333333333336
aF54.44444444444444
aF54.55555555555556
aF54.666666666666664
aF54.77777777777778
aF54.888888888888886
aF55.0
aF55.111111111111114
aF55.22222222222222
aF55.333333333333336
aF55.44444444444444
aF55.55555555555556
aF55.666666666666664
aF55.77777777777778
aF55.888888888888886
aF56.0
aF56.111111111111114
aF56.22222222222222
aF56.333333333333336
aF56.44444444444444
aF56.55555555555556
aF56.666666666666664
aF56.77777777777778
aF56.888888888888886
aF57.0
aF57.111111111111114
aF57.22222222222222
aF57.333333333333336
aF57.44444444444444
aF57.55555555555556
aF57.666666666666664
aF57.77777777777778
aF57.888888888888886
aF58.0
aF58.111111111111114
aF58.22222222222222
aF58.333333333333336
aF58.44444444444444
aF58.55555555555556
aF58.666666666666664
aF58.77777777777778
aF58.888888888888886
aF59.0
aF59.111111111111114
aF59.22222222222222
aF59.333333333333336
aF59.44444444444444
aF59.55555555555556
aF59.666666666666664
aF59.77777777777778
aF59.888888888888886
aF60.0
aF60.111111111111114
aF60.22222222222222
aF60.333333333333336
aF60.44444444444444
aF60.55555555555556
aF60.666666666666664
aF60.77777777777778
aF60.888888888888886
aF61.0
aF61.111111111111114
aF61.22222222222222
aF61.333333333333336
aF61.44444444444444
aF61.55555555555556
aF61.666666666666664
aF61.77777777777778
aF61.888888888888886
aF62.0
aF62.111111111111114
aF62.22222222222222
aF62.333333333333336
aF62.44444444444444
aF62.55555555555556
aF62.666666666666664
aF62.77777777777778
aF62.888888888888886
aF63.0
aF63.111111111111114
aF63.22222222222222
aF63.333333333333336
aF63.44444444444444
aF63.55555555555556
aF63.666666666666664
aF63.77777777777778
aF63.888888888888886
aF64.0
aF64.11111111111111
aF64.22222222222223
aF64.33333333333333
aF64.44444444444444
aF64.55555555555556
aF64.66666666666667
aF64.77777777777777
aF64.88888888888889
aF65.0
aF65.11111111111111
aF65.22222222222223
aF65.33333333333333
aF65.44444444444444
aF65.55555555555556
aF65.66666666666667
aF65.77777777777777
aF65.88888888888889
aF66.0
aF66.11111111111111
aF66.22222222222223
aF66.33333333333333
aF66.44444444444444
aF66.55555555555556
aF66.66666666666667
aF66.77777777777777
aF66.88888888888889
aF67.0
aF67.11111111111111
aF67.22222222222223
aF67.33333333333333
aF67.44444444444444
aF67.55555555555556
aF67.66666666666667
aF67.77777777777777
aF67.88888888888889
aF68.0
aF68.11111111111111
aF68.22222222222223
aF68.33333333333333
aF68.44444444444444
aF68.55555555555556
aF68.66666666666667
aF68.77777777777777
aF68.88888888888889
aF69.0
aF69.11111111111111
aF69.22222222222223
aF69.33333333333333
aF69.44444444444444
aF69.55555555555556
aF69.66666666666667
aF69.77777777777777
aF69.88888888888889
aF70.0
aF70.11111111111111
aF70.22222222222223
aF70.33333333333333
aF70.44444444444444
aF70.55555555555556
aF70.66666666666667
aF70.77777777777777
aF70.88888888888889
aF71.0
aF71.11111111111111
aF71.22222222222223
aF71.33333333333333
aF71.44444444444444
aF71.55555555555556
aF71.66666666666667
aF71.77777777777777
aF71.88888888888889
aF72.0
aF72.11111111111111
aF72.22222222222223
aF72.33333333333333
aF72.44444444444444
aF72.55555555555556
aF72.66666666666667
aF72.77777777777777
aF72.88888888888889
aF73.0
aF73.11111111111111
aF73.22222222222223
aF73.33333333333333
aF73.44444444444444
aF73.55555555555556
aF73.66666666666667
aF73.77777777777777
aF73.88888888888889
aF74.0
aF74.11111111111111
aF74.22222222222223
aF74.33333333333333
aF74.44444444444444
aF74.55555555555556
aF74.66666666666667
aF74.77777777777777
aF74.88888888888889
aF75.0
aF75.11111111111111
aF75.22222222222223
aF75.33333333333333
aF75.44444444444444
aF75.55555555555556
aF75.66666666666667
aF75.77777777777777
aF75.88888888888889
aF76.0
aF76.11111111111111
aF76.22222222222223
aF76.33333333333333
aF76.44444444444444
aF76.55555555555556
aF76.66666666666667
aF76.77777777777777
aF76.88888888888889
aF77.0
aF77.11111111111111
aF77.22222222222223
aF77.33333333333333
aF77.44444444444444
aF77.55555555555556
aF77.66666666666667
aF77.77777777777777
aF77.88888888888889
aF78.0
aF78.11111111111111
aF78.22222222222223
aF78.33333333333333
aF78.44444444444444
aF78.55555555555556
aF78.66666666666667
aF78.77777777777777
aF78.88888888888889
aF79.0
aF79.11111111111111
aF79.22222222222223
aF79.33333333333333
aF79.44444444444444
aF79.55555555555556
aF79.66666666666667
aF79.77777777777777
aF79.88888888888889
aF80.0
aF80.11111111111111
aF80.22222222222223
aF80.33333333333333
aF80.44444444444444
aF80.55555555555556
aF80.66666666666667
aF80.77777777777777
aF80.88888888888889
aF81.0
aF81.11111111111111
aF81.22222222222223
aF81.33333333333333
aF81.44444444444444
aF81.55555555555556
aF81.66666666666667
aF81.77777777777777
aF81.88888888888889
aF82.0
aF82.11111111111111
aF82.22222222222223
aF82.33333333333333
aF82.44444444444444
aF82.55555555555556
aF82.66666666666667
aF82.77777777777777
aF82.88888888888889
aF83.0
aF83.11111111111111
aF83.22222222222223
aF83.33333333333333
aF83.44444444444444
aF83.55555555555556
aF83.66666666666667
aF83.77777777777777
aF83.88888888888889
aF84.0
aF84.11111111111111
aF84.22222222222223
aF84.33333333333333
aF84.44444444444444
aF84.55555555555556
aF84.66666666666667
aF84.77777777777777
aF84.88888888888889
aF85.0
aF85.11111111111111
aF85.22222222222223
aF85.33333333333333
aF85.44444444444444
aF85.55555555555556
aF85.66666666666667
aF85.77777777777777
aF85.88888888888889
aF86.0
aF86.11111111111111
aF86.22222222222223
aF86.33333333333333
aF86.44444444444444
aF86.55555555555556
aF86.66666666666667
aF86.77777777777777
aF86.88888888888889
aF87.0
aF87.11111111111111
aF87.22222222222223
aF87.33333333333333
aF87.44444444444444
aF87.55555555555556
aF87.66666666666667
aF87.77777777777777
aF87.88888888888889
aF88.0
aF88.11111111111111
aF88.22222222222223
aF88.33333333333333
aF88.44444444444444
aF88.55555555555556
aF88.66666666666667
aF88.77777777777777
aF88.88888888888889
aF89.0
aF89.11111111111111
aF89.22222222222223
aF89.33333333333333
aF89.44444444444444
aF89.55555555555556
aF89.66666666666667
aF89.77777777777777
aF89.88888888888889
aF90.0
aF90.11111111111111
aF90.22222222222223
aF90.33333333333333
aF90.44444444444444
aF90.55555555555556
aF90.66666666666667
aF90.77777777777777
aF90.88888888888889
aF91.0
aF91.11111111111111
aF91.22222222222223
aF91.33333333333333
aF91.44444444444444
aF91.55555555555556
aF91.66666666666667
aF91.77777777777777
aF91.88888888888889
aF92.0
aF92.11111111111111
aF92.22222222222223
aF92.33333333333333
aF92.44444444444444
aF92.55555555555556
aF92.66666666666667
aF92.77777777777777
aF92.88888888888889
aF93.0
aF93.11111111111111
aF93.22222222222223
aF93.33333333333333
aF93.44444444444444
aF93.55555555555556
aF93.66666666666667
aF93.77777777777777
aF93.88888888888889
aF94.0
aF94.11111111111111
aF94.22222222222223
aF94.33333333333333
aF94.44444444444444
aF94.55555555555556
aF94.66666666666667
aF94.77777777777777
aF94.88888888888889
aF95.0
aF95.11111111111111
aF95.22222222222223
aF95.33333333333333
aF95.44444444444444
aF95.55555555555556
aF95.66666666666667
aF95.77777777777777
aF95.88888888888889
aF96.0
aF96.11111111111111
aF96.22222222222223
aF96.33333333333333
aF96.44444444444444
aF96.55555555555556
aF96.66666666666667
aF96.77777777777777
aF96.88888888888889
aF97.0
aF97.11111111111111
aF97.22222222222223
aF97.33333333333333
aF97.44444444444444
aF97.55555555555556
aF97.66666666666667
aF97.77777777777777
aF97.88888888888889
aF98.0
aF98.11111111111111
aF98.22222222222223
aF98.33333333333333
aF98.44444444444444
aF98.55555555555556
aF98.66666666666667
aF98.77777777777777
aF98.88888888888889
aF99.0
aF99.11111111111111
aF99.22222222222223
aF99.33333333333333
aF99.44444444444444
aF99.55555555555556
aF99.66666666666667
aF99.77777777777777
aF99.88888888888889
atp26
tp27
Rp28
aa(lp29
S'loss_bbox'
p30
ag0
(g22
g23
(S'L1Loss'
p31
(lp32
F0.235069
aF0.32846
aF0.315073
aF0.403167
aF0.93615
aF0.693082
aF0.986487
aF0.480465
aF0.433554
aF0.16969
aF0.597788
aF0.242211
aF0.618251
aF0.671214
aF0.566831
aF0.838162
aF0.571046
aF0.733044
aF0.558773
aF1.0543
aF0.302276
aF0.679577
aF1.10897
aF0.488299
aF0.550009
aF0.61407
aF0.646274
aF0.527066
aF0.482413
aF0.516094
aF0.772884
aF0.689252
aF0.754384
aF1.04216
aF0.684051
aF0.560362
aF1.00214
aF0.598909
aF0.953224
aF0.661988
aF0.7241
aF0.681346
aF0.450249
aF1.4696
aF0.671979
aF1.18376
aF0.773578
aF1.05585
aF0.559454
aF0.188384
aF0.569338
aF0.7127
aF0.165091
aF1.09457
aF1.04082
aF1.0304
aF1.11236
aF1.16796
aF0.642941
aF0.764606
aF1.27875
aF0.504966
aF0.956972
aF0.259184
aF0.488145
aF0.601472
aF0.692897
aF0.73857
aF0.758807
aF1.06656
aF0.327266
aF0.46365
aF0.223726
aF0.70772
aF0.896943
aF0.332541
aF0.32763
aF0.804861
aF0.407604
aF1.26296
aF0.563529
aF0.704961
aF0.482303
aF1.04533
aF0.601969
aF0.718057
aF0.940206
aF0.477848
aF0.878688
aF0.690799
aF0.841973
aF0.588738
aF0.63818
aF0.545193
aF0.72343
aF0.577621
aF0.673947
aF1.15703
aF0.413499
aF0.467799
aF1.13599
aF0.721276
aF0.472387
aF0.420373
aF0.168988
aF0.845492
aF0.339699
aF0.566038
aF1.19917
aF0.250618
aF1.12536
aF0.489821
aF0.559614
aF0.681296
aF0.853305
aF0.979985
aF0.542643
aF0.258468
aF0.597566
aF1.26079
aF0.865285
aF0.904781
aF0.589343
aF0.366452
aF0.716829
aF0.538609
aF1.4491
aF0.946993
aF0.898436
aF0.5423
aF0.532469
aF0.95994
aF0.444218
aF0.399492
aF0.685837
aF0.985064
aF0.516029
aF0.185298
aF0.904696
aF0.188061
aF0.509275
aF1.09031
aF0.188747
aF0.744973
aF0.82849
aF0.514091
aF0.521558
aF0.323811
aF0.486609
aF0.459633
aF0.499202
aF0.949713
aF0.367995
aF0.535253
aF0.65417
aF0.745254
aF1.15754
aF0.789134
aF0.344947
aF0.807532
aF0.623938
aF0.746038
aF0.602276
aF0.597956
aF1.25728
aF0.70953
aF0.566508
aF0.888529
aF0.841631
aF0.424912
aF0.408407
aF0.500267
aF0.557982
aF0.579341
aF0.932081
aF0.412894
aF0.655323
aF0.647953
aF0.150021
aF0.721537
aF0.333086
aF0.526933
aF1.0793
aF0.280991
aF0.629518
aF0.333673
aF0.485123
aF0.392296
aF0.602431
aF0.439586
aF0.460755
aF0.96956
aF0.304067
aF0.833718
aF0.295053
aF0.489564
aF0.840299
aF0.559863
aF0.436585
aF0.556067
aF0.208556
aF0.853119
aF0.405802
aF0.54471
aF0.893984
aF0.687354
aF0.377957
aF0.408788
aF0.444528
aF0.429883
aF0.17895
aF0.838868
aF0.191729
aF0.811173
aF0.317212
aF0.87882
aF0.338279
aF0.891066
aF0.536694
aF0.289359
aF0.366763
aF0.635313
aF0.52886
aF0.504332
aF0.385032
aF0.578026
aF1.0218
aF0.548793
aF0.863309
aF0.349516
aF0.341069
aF0.95401
aF0.486489
aF0.490006
aF0.647117
aF0.664459
aF0.193441
aF0.838719
aF0.783518
aF0.42226
aF0.539942
aF0.275924
aF1.31355
aF0.718083
aF0.907003
aF0.715972
aF0.70603
aF1.16213
aF0.946274
aF0.456332
aF0.732179
aF1.19047
aF0.695377
aF0.587493
aF0.518238
aF0.823433
aF0.177697
aF0.501528
aF0.325189
aF0.44463
aF0.400625
aF0.367843
aF1.07575
aF0.87952
aF1.13416
aF0.645275
aF0.222783
aF0.686237
aF0.363687
aF0.446436
aF0.998138
aF0.633345
aF0.761662
aF0.655743
aF0.460731
aF0.232928
aF0.868869
aF0.667154
aF0.781716
aF0.608073
aF0.937845
aF1.14691
aF0.427519
aF0.761766
aF0.331236
aF0.700334
aF0.519021
aF0.369135
aF0.239104
aF0.501073
aF0.481897
aF0.556943
aF0.441054
aF0.435255
aF0.309833
aF0.324324
aF0.722166
aF0.813347
aF0.623188
aF0.23653
aF0.420367
aF0.376248
aF0.147844
aF0.71287
aF0.658362
aF0.398852
aF0.222105
aF0.68638
aF0.162963
aF0.168793
aF0.803253
aF1.9144
aF0.544199
aF0.918015
aF0.582631
aF0.866602
aF0.653472
aF0.53355
aF1.0802
aF0.428317
aF0.440064
aF0.937802
aF0.730685
aF0.693614
aF0.611713
aF0.206601
aF0.811748
aF0.663575
aF1.02496
aF0.616593
aF1.35578
aF0.885853
aF0.469065
aF0.390992
aF0.525383
aF0.585641
aF0.854085
aF0.7413
aF0.56654
aF0.551285
aF0.858874
aF0.530982
aF0.445154
aF0.507339
aF0.668147
aF0.518833
aF0.172797
aF0.488015
aF0.444741
aF0.367638
aF0.3762
aF0.392522
aF0.613414
aF0.379519
aF0.188709
aF0.330896
aF0.706254
aF0.434131
aF0.34301
aF0.158726
aF0.524379
aF0.705754
aF0.465316
aF1.02424
aF1.12882
aF0.551975
aF0.170743
aF1.26353
aF0.629438
aF0.363808
aF0.349092
aF0.755297
aF0.755528
aF0.823751
aF0.287505
aF0.366676
aF0.582611
aF0.271791
aF0.360753
aF0.259781
aF1.10028
aF0.768775
aF0.655805
aF0.664328
aF0.602413
aF0.195047
aF0.607446
aF0.777626
aF0.166826
aF0.721825
aF0.627695
aF0.112096
aF0.158833
aF0.681266
aF0.89888
aF0.396115
aF0.496187
aF0.597437
aF0.595979
aF0.414252
aF0.40253
aF0.31892
aF0.283407
aF0.69828
aF0.480542
aF0.300974
aF0.861312
aF0.416105
aF0.447844
aF0.51689
aF0.516974
aF0.195049
aF0.164063
aF0.44829
aF0.636357
aF0.819897
aF0.757028
aF0.56713
aF0.597534
aF0.325298
aF0.387495
aF0.791358
aF0.282802
aF0.250692
aF0.681471
aF0.569445
aF0.370998
aF0.425159
aF0.856164
aF0.723369
aF0.549456
aF0.570514
aF0.537211
aF0.510964
aF1.11405
aF0.346395
aF0.826947
aF0.38768
aF0.352827
aF0.603023
aF0.401728
aF0.26588
aF0.785865
aF1.02678
aF0.539591
aF0.33852
aF0.888086
aF0.432067
aF0.653063
aF0.474068
aF0.434946
aF0.711267
aF0.159851
aF0.337847
aF0.416044
aF0.58609
aF0.316944
aF0.519781
aF0.597265
aF0.743107
aF0.458489
aF0.256059
aF0.749133
aF0.531649
aF0.453186
aF0.281774
aF0.74158
aF0.37124
aF0.36055
aF1.18964
aF0.877758
aF0.362214
aF0.411172
aF0.723383
aF0.84978
aF0.487648
aF0.316936
aF0.534277
aF0.601647
aF0.52433
aF0.801134
aF0.725586
aF0.50549
aF0.351827
aF0.432733
aF0.820323
aF0.703847
aF0.494849
aF0.470504
aF0.548002
aF0.510497
aF0.399391
aF0.535592
aF0.882976
aF0.547566
aF0.288758
aF0.521881
aF0.726897
aF1.07254
aF0.48943
aF0.43884
aF0.54563
aF0.73323
aF0.936007
aF0.516596
aF0.538803
aF0.692401
aF0.444234
aF0.255984
aF0.431914
aF0.615954
aF0.127518
aF0.611853
aF0.411001
aF0.583505
aF0.909567
aF0.678272
aF1.25944
aF0.43578
aF0.483542
aF0.31212
aF0.352141
aF0.575795
aF0.350239
aF0.295316
aF0.81684
aF0.371618
aF0.199084
aF0.562111
aF0.130949
aF0.822699
aF0.54505
aF0.571978
aF0.295564
aF0.681145
aF1.08947
aF0.370062
aF0.479284
aF0.408444
aF0.612793
aF0.631187
aF0.40889
aF0.756706
aF0.630703
aF0.821849
aF0.454403
aF0.525902
aF0.147736
aF0.711038
aF0.730114
aF0.651497
aF0.993321
aF1.08591
aF0.180983
aF0.495715
aF0.697524
aF0.398254
aF0.359138
aF0.740686
aF0.531565
aF0.623736
aF0.204828
aF0.839578
aF0.186936
aF0.553644
aF1.11697
aF0.774128
aF1.27719
aF0.49962
aF1.04131
aF0.539758
aF0.496668
aF0.594015
aF0.631442
aF0.342858
aF0.971495
aF0.370177
aF0.173928
aF0.347168
aF0.341477
aF0.546378
aF0.309394
aF0.292046
aF0.315439
aF0.492532
aF0.617347
aF0.436779
aF1.23032
aF0.224552
aF0.42584
aF0.311941
aF0.64296
aF0.299253
aF0.51908
aF0.623765
aF0.884465
aF0.366513
aF0.712871
aF0.221136
aF0.447466
aF0.320747
aF0.577861
aF0.588018
aF0.330159
aF0.790014
aF0.411088
aF0.521152
aF1.54007
aF0.781806
aF0.367931
aF0.321689
aF0.555356
aF0.704646
aF0.352492
aF0.216776
aF0.17793
aF0.426426
aF0.607426
aF0.400304
aF0.887409
aF0.262254
aF0.782973
aF0.571256
aF0.558264
aF0.339914
aF0.472686
aF0.930959
aF0.361069
aF0.485694
aF0.488896
aF0.397062
aF0.33132
aF1.07887
aF0.616551
aF0.810041
aF0.616057
aF0.166226
aF0.265787
aF0.71073
aF0.423898
aF0.97086
aF0.312205
aF0.451225
aF0.460366
aF0.654794
aF0.300144
aF1.21192
aF0.547557
aF0.433569
aF0.95352
aF0.732054
aF0.476416
aF0.381562
aF0.358972
aF0.856419
aF0.173624
aF0.64928
aF0.437356
aF0.706179
aF0.667984
aF0.754487
aF0.602867
aF0.351507
aF0.121621
aF0.540348
aF0.271034
aF0.571149
aF0.341803
aF0.578042
aF0.52515
aF0.31285
aF0.512877
aF0.423328
aF0.592139
aF0.534826
aF0.53696
aF0.56724
aF0.536497
aF0.330248
aF0.584655
aF0.856213
aF0.28446
aF0.351383
aF0.595631
aF0.479107
aF0.636901
aF0.274667
aF0.568957
aF0.930764
aF0.382374
aF0.921077
aF0.579748
aF0.711786
aF0.538071
aF0.532982
aF0.368366
aF0.411059
aF0.899062
aF0.637141
aF0.8088
aF0.763255
aF0.509452
aF0.303101
aF0.713132
aF0.471627
aF0.48804
aF0.43916
aF0.416925
aF0.907305
aF0.581144
aF0.376235
aF0.773763
aF0.743946
aF0.31592
aF0.39189
aF0.77842
aF0.913922
aF0.662575
aF0.380825
aF0.79794
aF0.503093
aF0.532635
aF0.850523
aF0.825638
aF0.239841
aF0.501006
aF0.408955
aF0.610453
aF0.19039
aF0.536325
aF0.68886
aF0.40787
aF0.218403
aF0.172478
aF0.289869
aF0.360971
aF0.546413
aF0.288725
aF0.534437
aF0.392579
aF0.904059
aF0.840243
aF0.255713
aF0.353442
aF0.924656
aF1.02366
aF0.405505
aF0.310898
aF0.553016
aF0.381972
aF0.134876
aF0.498725
aF0.428093
aF0.402377
aF0.470736
aF0.252175
aF1.03904
aF0.275995
aF0.45663
aF0.536277
aF0.619029
aF0.911739
aF0.139737
aF0.37546
aF0.383826
aF0.744631
aF0.25168
aF1.50503
aF0.802516
aF0.234041
aF0.333597
aF0.313402
aF0.621342
aF0.871502
aF0.798644
aF0.412032
aF0.64388
aF1.24924
aF0.128193
aF0.605412
aF0.439105
aF0.482749
aF0.506231
aF0.770269
aF0.161033
aF0.470447
aF0.376645
aF0.728595
aF0.562416
aF0.39997
aF0.752551
aF0.417308
aF0.528262
aF0.38385
aF0.738765
aF0.429734
aF0.564381
aF1.05861
aF0.193602
aF0.736039
aF0.330832
aF0.389512
aF0.666713
aF0.649181
aF0.375354
aF0.905883
aF0.149996
aF0.359763
aF0.868338
aF0.548051
aF0.229656
aF0.555552
aF0.429382
aF0.260995
aF0.662355
aF0.665297
aF0.806849
aF0.313744
aF0.536937
aF0.705627
aF0.238597
aF0.453188
aF0.454098
aF0.736807
aF0.177879
aF0.313608
aF0.670041
aF0.439238
aF0.811913
aF0.306212
aF0.306482
aF0.551355
aF0.493735
aF0.524744
aF0.374214
aF0.649179
aF0.418862
aF0.575649
aF1.0541
aF0.311541
aF0.637742
aF0.450282
aF0.692752
aF0.404085
aF0.545469
aF0.137397
aF0.54793
aF0.581633
aF0.506385
aF0.321212
aF0.662366
aF0.440271
aF0.440968
aF0.663947
aF0.47918
aF1.27762
aF0.18653
aF0.328131
aF0.570533
aF0.633566
aF0.638047
aF0.810608
aF0.339672
aF0.322647
aF0.765224
aF0.396431
aF0.706776
aF0.437218
aF0.363077
aF0.413474
aF0.31715
aF0.329986
aF0.511701
aF0.122712
aF0.164739
aF0.399398
aF0.337997
aF0.485411
aF0.139397
aF0.697712
aF0.1501
aF0.582531
aF0.422959
aF0.459927
aF0.572587
aF0.583576
aF0.383511
aF0.541923
aF0.894015
aF0.472403
aF0.410524
aF0.328975
aF0.405148
aF0.473137
aF0.67341
aF0.267745
aF0.598905
aF1.09327
aF0.657465
aF0.306039
atp33
tp34
Rp35
aa(lp36
S'loss_coverage'
p37
ag0
(g22
g23
(S'EuclideanLoss'
p38
(lp39
F0.630459
aF11.0103
aF0.000593404
aF1.95744
aF1.32333
aF0.607642
aF3.84835
aF0.608783
aF0.771916
aF0.478062
aF2.48379
aF0.00041152
aF0.69405
aF1.96732
aF0.00311763
aF7.68749e-05
aF0.00940111
aF0.0360114
aF4.70937
aF0.558698
aF1.15102e-05
aF2.25751
aF20.4948
aF0.00145613
aF0.00283111
aF3.83895
aF9.95703
aF1.28317e-06
aF1.76245
aF1.38087
aF3.88522
aF1.07463
aF0.296075
aF1.83717
aF0.0662915
aF3.1755
aF0.231074
aF1.10985
aF0.466567
aF7.01807
aF1.42304
aF0.000186761
aF0.000301883
aF0.357177
aF0.503265
aF1.08838
aF0.639187
aF0.010791
aF0.000886057
aF0.000203517
aF0.00270098
aF2.81497
aF1.30355
aF0.600262
aF1.9983
aF0.858351
aF3.13912
aF0.487421
aF0.266321
aF0.714323
aF8.41533e-05
aF0.537291
aF1.666
aF8.8537e-07
aF0.000362703
aF0.232072
aF0.584912
aF2.4888
aF0.43954
aF4.56715
aF1.2754
aF0.0256962
aF1.11744
aF0.311271
aF0.274164
aF0.166272
aF20.8752
aF9.836
aF0.238493
aF0.282852
aF0.232743
aF2.82411
aF0.0285147
aF0.691781
aF7.76048
aF0.918405
aF0.0843088
aF2.49844
aF0.189597
aF1.57065
aF6.1845
aF1.20444
aF0.552347
aF1.32436
aF4.03816
aF1.88678
aF0.270881
aF4.01517
aF0.000723024
aF2.01728
aF0.411363
aF4.16118
aF0.141103
aF1.91106
aF0.542083
aF0.0922356
aF0.000359816
aF15.6402
aF0.30862
aF0.000289484
aF2.63126
aF0.48414
aF0.000111794
aF0.142287
aF8.17512
aF0.115846
aF0.00216524
aF0.0411958
aF0.000608417
aF0.807075
aF0.300379
aF0.775711
aF0.537864
aF2.20073
aF2.34051
aF0.0355131
aF0.813949
aF0.00122678
aF0.0109704
aF2.32728
aF2.23397
aF2.18482
aF0.91219
aF3.16456
aF0.768816
aF1.8641
aF2.79096
aF10.0618
aF0.0682632
aF1.92254e-07
aF2.93029
aF2.47445
aF1.0033
aF0.655293
aF3.06919
aF0.444497
aF2.36483
aF0.141347
aF0.42559
aF1.17866
aF0.00729159
aF0.601806
aF0.00222867
aF1.30416
aF9.858
aF0.573036
aF2.7489
aF0.839184
aF5.76379
aF1.41732
aF0.799632
aF0.00472862
aF0.00109985
aF0.0458064
aF0.563779
aF0.00710397
aF0.475176
aF1.76326
aF0.573483
aF0.431474
aF0.547711
aF1.31383
aF0.346734
aF1.66781
aF0.65011
aF3.10852
aF0.438308
aF0.159465
aF0.755538
aF1.96716
aF4.52408e-05
aF0.0312298
aF0.763244
aF2.06403
aF1.24004
aF0.36314
aF0.0440818
aF1.78021
aF0.0159875
aF0.0257718
aF1.7553
aF0.5072
aF1.01595e-05
aF1.11365
aF0.242555
aF0.486807
aF0.661453
aF0.738647
aF1.04471
aF22.6921
aF0.00128581
aF8.65461
aF0.0138791
aF0.738552
aF0.946716
aF0.167105
aF0.000153808
aF2.16984
aF0.288258
aF0.31559
aF0.777578
aF5.89781
aF1.28052e-05
aF1.16635
aF0.41388
aF2.20592
aF0.261144
aF1.48533
aF0.0116534
aF0.0271616
aF0.905147
aF0.0408814
aF0.374744
aF8.39416
aF0.000560099
aF4.47431
aF0.560879
aF0.000952089
aF1.376
aF1.42365
aF2.14196
aF2.12066
aF2.11502
aF1.95167
aF0.0230885
aF2.26452
aF0.000235261
aF3.09627
aF0.0251615
aF1.64323
aF0.260798
aF0.0939367
aF2.28279
aF7.79352
aF2.9573
aF3.34821
aF0.615953
aF0.608172
aF3.03527
aF0.789164
aF1.03547
aF13.5015
aF6.75618
aF5.84398
aF0.0981247
aF0.457683
aF0.531598
aF0.0636908
aF0.441428
aF1.30907
aF0.0416828
aF0.00124144
aF0.238802
aF2.59129
aF0.0434708
aF2.6979
aF0.356497
aF1.69413
aF0.0917893
aF0.168769
aF4.19503
aF0.599524
aF0.449446
aF0.311633
aF0.0160866
aF1.78439
aF0.0388286
aF0.00102764
aF0.00883951
aF0.520759
aF0.865439
aF2.08738
aF3.74568e-05
aF1.05369
aF6.4062
aF1.41141
aF0.0184635
aF7.56625
aF1.15806
aF0.0361943
aF4.32633
aF0.00746922
aF1.93846
aF0.197686
aF0.0507852
aF3.14706
aF1.92925
aF0.0283822
aF0.16734
aF3.21612e-06
aF1.03036
aF0.000507423
aF0.000157347
aF0.00965413
aF2.28869
aF0.482898
aF0.0202724
aF0.00607119
aF4.97461
aF0.000988339
aF8.11277
aF2.18833
aF1.53803
aF1.3042
aF0.399961
aF0.554979
aF1.48708
aF1.40661
aF0.0750033
aF3.99993
aF8.93972
aF0.309563
aF13.1672
aF1.99663
aF0.0707503
aF0.0495071
aF2.18646
aF4.07506
aF3.23472
aF0.642573
aF2.53252
aF1.46775
aF0.00325706
aF0.398518
aF2.8988
aF0.254193
aF0.233269
aF3.53137
aF0.0118757
aF0.0151355
aF10.2477
aF0.376482
aF2.11963
aF3.11961
aF0.778028
aF0.0196552
aF5.13955e-05
aF1.09545
aF0.0036256
aF2.05472
aF0.070579
aF2.93153
aF0.000876451
aF0.981038
aF0.0569755
aF0.0237659
aF0.202169
aF0.00222137
aF0.764211
aF0.5013
aF1.36272
aF5.76628
aF1.31594
aF0.0706044
aF1.26992
aF0.673161
aF0.0262244
aF1.80134
aF6.19078
aF2.94531
aF6.70288
aF0.346919
aF0.0164165
aF0.000248356
aF0.481947
aF0.493488
aF0.310552
aF0.359876
aF0.343979
aF2.80546
aF0.252392
aF0.402308
aF4.60864
aF0.00612486
aF0.126998
aF7.93728
aF1.12499
aF0.879695
aF0.495616
aF1.91723
aF0.0824315
aF0.00646204
aF0.809225
aF0.0492201
aF0.244632
aF0.00323304
aF0.0970487
aF2.07579
aF0.693377
aF1.42355
aF0.0492701
aF0.00029786
aF0.0790113
aF0.999088
aF0.0819097
aF0.835157
aF1.01829
aF1.60444
aF0.000649584
aF0.223166
aF0.458015
aF0.00162908
aF1.88497
aF0.812876
aF0.501072
aF0.65271
aF0.128822
aF0.0287678
aF2.49263
aF1.70362
aF0.35055
aF0.163716
aF0.000544737
aF0.0156028
aF0.0137657
aF0.224183
aF0.0644396
aF0.916719
aF1.61555
aF0.876116
aF0.0584357
aF1.74167
aF0.0179943
aF0.144111
aF0.897659
aF0.0019117
aF0.691709
aF1.16912
aF0.305994
aF0.297769
aF8.38817
aF1.46001
aF0.332129
aF2.31694
aF0.00713117
aF0.0263477
aF2.37539
aF1.61534
aF0.535362
aF0.562806
aF0.313726
aF0.108403
aF0.427198
aF3.36856
aF0.00772218
aF1.33667
aF0.0124777
aF2.01648
aF0.167157
aF0.317382
aF2.05022
aF0.526833
aF0.0115712
aF0.63567
aF0.111487
aF0.00507547
aF0.430384
aF0.522565
aF0.785785
aF0.894229
aF0.000569333
aF3.79221
aF0.000172271
aF3.53884
aF0.0869419
aF0.477404
aF1.43027
aF0.79885
aF2.20823
aF0.0113787
aF0.23092
aF0.0377523
aF3.11518
aF0.172233
aF3.69827
aF5.81624
aF3.00404
aF0.159048
aF0.727306
aF0.61084
aF1.23477
aF0.0260889
aF0.565358
aF0.00948761
aF1.26285
aF3.70208
aF0.362623
aF7.38499
aF0.775759
aF0.347378
aF0.0458672
aF0.132645
aF1.57383
aF0.15957
aF0.0529798
aF0.665477
aF0.497674
aF0.026312
aF0.000289518
aF0.128903
aF0.000310988
aF5.92712e-05
aF0.0468852
aF0.443663
aF0.280586
aF1.45712
aF0.867866
aF0.0919572
aF0.10794
aF0.00289055
aF0.218798
aF0.00445903
aF0.0125017
aF3.09093
aF1.22226
aF0.758372
aF0.00162329
aF0.0286237
aF0.586736
aF0.635536
aF0.656124
aF0.00528139
aF0.00234722
aF1.83702
aF1.7563
aF2.58983
aF0.358765
aF1.28402
aF0.256906
aF0.376121
aF0.690257
aF0.0448853
aF0.726274
aF0.982043
aF0.000283729
aF1.52133
aF0.684966
aF7.48002e-05
aF0.000427838
aF0.236552
aF0.968467
aF0.345601
aF1.9374
aF2.44673
aF0.894102
aF0.102772
aF0.228463
aF1.30925
aF0.205227
aF3.39488
aF1.11138
aF0.0530286
aF4.17315
aF0.020761
aF0.990453
aF2.96154
aF3.72885
aF1.25294
aF0.0465109
aF0.652355
aF0.0146682
aF0.805153
aF1.43672
aF1.32723
aF0.0905647
aF0.0869911
aF0.000516687
aF0.0528205
aF0.0338113
aF1.2016
aF0.35481
aF1.09967
aF1.03963
aF0.790001
aF0.0928099
aF0.274667
aF0.299393
aF0.485431
aF0.738408
aF0.993367
aF0.0488707
aF0.21268
aF0.961641
aF0.00107858
aF0.389407
aF0.013518
aF0.00504046
aF1.07964
aF0.000277074
aF0.224798
aF0.425797
aF0.276376
aF0.290345
aF3.59298
aF0.175713
aF3.24738
aF1.08767
aF0.728823
aF3.87656
aF2.4131e-05
aF0.50532
aF0.0047134
aF1.30197
aF0.431162
aF2.01528
aF0.000169582
aF3.03467
aF0.00289453
aF0.652775
aF0.971684
aF0.0309289
aF1.48456
aF2.09529
aF1.26676
aF0.0010139
aF0.0137233
aF0.0845414
aF0.0954917
aF2.97406
aF1.53671
aF0.434739
aF0.337258
aF0.807082
aF0.155257
aF0.110721
aF2.08971
aF0.0336363
aF1.30291
aF0.763244
aF2.35382
aF0.791346
aF0.0386759
aF0.000236321
aF0.430678
aF0.122537
aF0.390223
aF1.05995
aF0.0305835
aF1.47575
aF0.0187349
aF0.555517
aF1.06242
aF0.000424028
aF0.15479
aF0.111519
aF0.510146
aF0.0888709
aF0.00475235
aF0.323402
aF0.450291
aF0.0593467
aF1.56765
aF0.287506
aF0.00134848
aF1.38649
aF2.85072
aF2.1986
aF1.39698
aF0.0327971
aF0.985251
aF0.55992
aF1.63729
aF0.0148502
aF0.0689178
aF0.374316
aF0.963848
aF0.554974
aF1.20697
aF3.53611
aF0.28923
aF0.0063441
aF0.035107
aF0.855877
aF2.08578
aF0.617783
aF0.473914
aF0.120018
aF1.14715
aF0.09101
aF0.0493113
aF1.91744
aF2.11912
aF1.66607
aF0.00984095
aF0.00164876
aF0.067681
aF0.538881
aF0.260827
aF0.00128921
aF1.31512
aF3.07159
aF0.233486
aF1.26958e-05
aF2.75691
aF0.206206
aF3.07642
aF0.00642798
aF0.774019
aF0.00163583
aF0.00491603
aF0.271102
aF0.334106
aF0.739631
aF0.0188796
aF0.0250024
aF0.0436021
aF1.2783
aF1.39437
aF0.344282
aF0.0165241
aF0.97002
aF0.276488
aF0.262867
aF2.80602
aF0.122662
aF0.00956205
aF0.00728334
aF0.00277469
aF0.00200126
aF0.208657
aF0.0216001
aF1.64668
aF0.000398533
aF7.95486
aF3.32282
aF1.68202
aF0.998253
aF1.04476
aF0.0348341
aF1.20456
aF1.67668
aF0.0125753
aF1.9553e-05
aF0.00421459
aF3.80031
aF0.911149
aF0.0169078
aF0.614273
aF0.753245
aF0.00608436
aF0.00193631
aF0.205376
aF0.0422607
aF0.00742979
aF1.42831
aF0.292204
aF0.636151
aF0.269279
aF0.0445914
aF2.63013
aF0.290122
aF2.13587
aF0.112105
aF0.000749652
aF0.773091
aF0.393141
aF0.00254865
aF1.85387
aF0.166669
aF1.017
aF0.0348768
aF0.00860095
aF1.73093
aF0.167732
aF0.105104
aF7.69175
aF0.0460936
aF0.878374
aF0.255907
aF1.48019
aF0.0443208
aF0.184126
aF1.13207
aF0.000367066
aF0.197783
aF0.00747664
aF0.671903
aF0.323007
aF0.00542432
aF1.32728
aF0.143973
aF0.036961
aF0.349964
aF0.00139546
aF0.165244
aF0.208606
aF0.257066
aF1.15985
aF0.243366
aF0.0497861
aF0.27362
aF0.569569
aF0.466791
aF0.300674
aF0.833914
aF0.0360244
aF0.0354349
aF1.6064
aF0.0591623
aF0.45183
aF0.229248
aF1.47104
aF0.535119
aF6.85414
aF0.000568069
aF1.09883
aF10.0044
aF3.91193
aF0.240713
aF0.00706483
aF0.000610579
aF3.71177
aF0.0170742
aF0.41596
aF0.0164713
aF0.00226116
aF1.99623
aF0.444584
aF0.991849
aF5.03268
aF1.25205
aF0.805372
aF0.000308364
aF0.468478
aF4.28371
aF0.599149
aF0.869267
aF1.03757
aF1.05935
aF0.1608
aF0.0807167
aF0.0293258
aF3.34609
aF0.666572
aF0.0601847
aF0.0761145
aF0.0206242
aF0.00130986
aF0.0174411
aF0.512365
aF0.0774127
aF0.00180098
aF0.704695
aF0.0230298
aF0.736797
aF1.04456
aF2.54453e-05
aF0.0244059
aF0.320909
aF0.21123
aF0.195291
aF0.386603
aF1.89138
aF0.396276
aF3.54529
aF1.67071
aF0.00047475
aF0.0316736
aF1.60749
aF0.000621954
aF1.51229
aF1.14763
aF0.00029784
aF1.88247
aF0.0269235
aF1.99071
aF0.0497759
aF0.000360492
aF0.0317282
aF0.0256786
aF0.0332162
aF0.545361
aF0.0163576
aF2.84129
aF0.929808
aF1.38692
aF0.28594
aF0.314654
aF0.495426
aF0.377489
aF0.010647
aF0.0357698
aF1.57351
aF0.00458119
aF1.88784
aF0.0942228
aF7.72268
aF1.4011
aF1.33591
aF0.0957871
atp40
tp41
Rp42
aa(lp43
S'learning_rate'
p44
ag0
(g22
g23
(S'LearningRate'
p45
(lp46
F0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
aF0.0001
atp47
tp48
Rp49
aatp50
Rp51
sS'val_outputs'
p52
g18
((lp53
(lp54
g21
ag0
(g22
g23
(g24
(lp55
I0
aF1.0
aF2.0
aF3.0
aF4.0
aF5.0
aF6.0
aF7.0
aF8.0
aF9.0
aF10.0
aF11.0
aF12.0
aF13.0
aF14.0
aF15.0
aF16.0
aF17.0
aF18.0
aF19.0
aF20.0
aF21.0
aF22.0
aF23.0
aF24.0
aF25.0
aF26.0
aF27.0
aF28.0
aF29.0
aF30.0
aF31.0
aF32.0
aF33.0
aF34.0
aF35.0
aF36.0
aF37.0
aF38.0
aF39.0
aF40.0
aF41.0
aF42.0
aF43.0
aF44.0
aF45.0
aF46.0
aF47.0
aF48.0
aF49.0
aF50.0
aF51.0
aF52.0
aF53.0
aF54.0
aF55.0
aF56.0
aF57.0
aF58.0
aF59.0
aF60.0
aF61.0
aF62.0
aF63.0
aF64.0
aF65.0
aF66.0
aF67.0
aF68.0
aF69.0
aF70.0
aF71.0
aF72.0
aF73.0
aF74.0
aF75.0
aF76.0
aF77.0
aF78.0
aF79.0
aF80.0
aF81.0
aF82.0
aF83.0
aF84.0
aF85.0
aF86.0
aF87.0
aF88.0
aF89.0
aF90.0
aF91.0
aF92.0
aF93.0
aF94.0
aF95.0
aF96.0
aF97.0
aF98.0
aF99.0
aF100.0
atp56
tp57
Rp58
aa(lp59
S'loss_bbox'
p60
ag0
(g22
g23
(S'L1Loss'
p61
(lp62
F0.513068
aF0.497564
aF0.478929
aF0.395233
aF0.425254
aF0.518551
aF0.42524
aF0.416526
aF0.419643
aF0.379722
aF0.435102
aF0.422581
aF0.368719
aF0.371185
aF0.38506
aF0.398778
aF0.34552
aF0.393912
aF0.363256
aF0.352754
aF0.400413
aF0.367527
aF0.353788
aF0.366752
aF0.334288
aF0.339792
aF0.357158
aF0.479532
aF0.335717
aF0.330787
aF0.384074
aF0.326011
aF0.380307
aF0.338671
aF0.353452
aF0.338633
aF0.345804
aF0.349533
aF0.367186
aF0.32545
aF0.326578
aF0.31029
aF0.341691
aF0.302641
aF0.327702
aF0.332097
aF0.342408
aF0.313066
aF0.298215
aF0.297827
aF0.289991
aF0.299808
aF0.285573
aF0.306321
aF0.337535
aF0.281389
aF0.293557
aF0.344709
aF0.321214
aF0.314588
aF0.367317
aF0.295829
aF0.314303
aF0.333574
aF0.307289
aF0.285205
aF0.301034
aF0.285196
aF0.308471
aF0.287672
aF0.271937
aF0.289945
aF0.272905
aF0.294073
aF0.306544
aF0.3385
aF0.29263
aF0.304185
aF0.296344
aF0.345468
aF0.312731
aF0.30553
aF0.291316
aF0.259741
aF0.273001
aF0.270176
aF0.287927
aF0.30729
aF0.288707
aF0.279407
aF0.281693
aF0.274919
aF0.294817
aF0.358297
aF0.253456
aF0.281878
aF0.338072
aF0.277276
aF0.285793
aF0.310145
aF0.306745
atp63
tp64
Rp65
aa(lp66
S'loss_coverage'
p67
ag0
(g22
g23
(S'EuclideanLoss'
p68
(lp69
F2.28895
aF2.20109
aF2.49374
aF2.13192
aF2.25704
aF2.21309
aF2.05125
aF2.18181
aF1.89005
aF2.02991
aF2.01632
aF2.13317
aF1.98496
aF1.8534
aF1.89998
aF2.12416
aF1.97347
aF1.92971
aF1.69756
aF1.86629
aF1.77178
aF1.98964
aF1.67089
aF1.56479
aF1.80459
aF1.68729
aF1.62776
aF1.80884
aF1.73241
aF1.6971
aF1.73508
aF1.6173
aF1.70404
aF1.44445
aF1.57164
aF1.66919
aF1.78844
aF1.45478
aF1.371
aF1.49255
aF1.54131
aF1.36089
aF1.50979
aF1.46063
aF1.59131
aF1.47501
aF1.41737
aF1.50943
aF1.29822
aF1.34001
aF1.37613
aF1.3845
aF1.20811
aF1.31607
aF1.40312
aF1.5126
aF1.27335
aF1.46643
aF1.44094
aF1.41901
aF1.3939
aF1.24564
aF1.32843
aF1.30596
aF1.25247
aF1.28208
aF1.31812
aF1.35763
aF1.32048
aF1.33214
aF1.21217
aF1.24623
aF1.2536
aF1.27987
aF1.24371
aF1.19861
aF1.21511
aF1.16086
aF1.17125
aF1.23361
aF1.23862
aF1.29567
aF1.18833
aF1.22534
aF1.19291
aF1.26816
aF1.35149
aF1.3077
aF1.15466
aF1.16363
aF1.29035
aF1.11666
aF1.34828
aF1.27436
aF1.24025
aF1.07315
aF1.225
aF1.18846
aF1.32363
aF1.2139
aF1.25021
atp70
tp71
Rp72
aa(lp73
S'mAP'
p74
ag0
(g22
g23
(S'Python'
p75
(lp76
F92.4646
aF93.0607
aF94.6947
aF94.3388
aF93.8419
aF93.6227
aF94.0541
aF94.3547
aF94.9685
aF94.2297
aF93.8344
aF94.8327
aF94.5581
aF95.6024
aF94.9482
aF94.3988
aF95.1188
aF95.3827
aF94.9406
aF94.8753
aF94.8566
aF94.703
aF95.5419
aF95.7301
aF95.7699
aF95.1337
aF95.4663
aF95.5296
aF95.2867
aF94.8661
aF95.1869
aF95.6432
aF95.0386
aF95.6966
aF95.803
aF95.7948
aF96.0874
aF96.477
aF96.2144
aF95.5594
aF95.1574
aF95.4234
aF95.8415
aF96.2121
aF94.9257
aF95.5054
aF95.6698
aF95.93
aF95.2707
aF95.8601
aF95.9976
aF95.9269
aF96.4149
aF95.7802
aF95.9678
aF95.9547
aF95.5354
aF96.3091
aF94.6618
aF96.2489
aF95.3879
aF95.9479
aF96.0855
aF96.2636
aF95.7317
aF95.8871
aF95.3846
aF96.0914
aF95.8259
aF96.2934
aF96.1761
aF96.232
aF95.656
aF96.2422
aF96.2179
aF96.2312
aF95.7225
aF95.5111
aF96.119
aF96.0505
aF95.9512
aF95.7731
aF96.0248
aF96.2104
aF96.3399
aF96.1191
aF95.8535
aF95.5533
aF95.6016
aF95.6497
aF95.8859
aF96.3679
aF96.079
aF95.0836
aF96.0041
aF95.8018
aF95.8736
aF95.8899
aF96.4909
aF95.8628
aF96.1417
atp77
tp78
Rp79
aa(lp80
S'precision'
p81
ag0
(g22
g23
(S'Python'
p82
(lp83
F94.9693
aF95.889
aF97.1362
aF96.7068
aF96.4634
aF96.1958
aF96.4243
aF96.8692
aF97.1611
aF96.6554
aF96.2585
aF97.1969
aF97.0136
aF98.0098
aF97.177
aF96.8319
aF97.6689
aF98.1859
aF97.5192
aF97.5317
aF97.0354
aF97.0915
aF97.8754
aF98.1317
aF98.1014
aF97.5161
aF97.7632
aF97.9056
aF97.7134
aF97.4503
aF97.6273
aF98.1347
aF97.7805
aF98.171
aF98.3181
aF98.3445
aF98.5828
aF98.7379
aF98.4025
aF98.0062
aF97.6282
aF97.8285
aF98.0019
aF98.7468
aF97.597
aF97.8382
aF98.1914
aF98.0657
aF97.7048
aF98.1751
aF98.1639
aF98.3322
aF98.7391
aF98.419
aF98.3487
aF98.1902
aF97.98
aF98.5778
aF97.2703
aF98.7611
aF97.7072
aF98.5654
aF98.5131
aF98.5329
aF98.2649
aF98.4962
aF98.0594
aF98.4676
aF98.5308
aF98.7202
aF98.6548
aF98.4939
aF98.0413
aF98.6143
aF98.7516
aF98.7142
aF98.1525
aF98.114
aF98.447
aF98.6258
aF98.6097
aF98.3038
aF98.6975
aF98.7015
aF98.8378
aF98.5574
aF98.3985
aF97.9245
aF98.1861
aF98.0968
aF98.2581
aF98.6353
aF98.5102
aF97.524
aF98.6474
aF98.1786
aF98.399
aF98.5033
aF99.1588
aF98.5084
aF98.6227
atp84
tp85
Rp86
aa(lp87
S'recall'
p88
ag0
(g22
g23
(S'Python'
p89
(lp90
F96.9583
aF96.6968
aF97.3906
aF97.2705
aF96.9841
aF97.0099
aF97.3137
aF97.1611
aF97.6316
aF97.2529
aF97.2601
aF97.3974
aF97.1665
aF97.438
aF97.5139
aF97.3117
aF97.2837
aF96.9947
aF97.1752
aF97.0915
aF97.5979
aF97.2824
aF97.5152
aF97.5133
aF97.5769
aF97.4205
aF97.5768
aF97.489
aF97.4275
aF97.1926
aF97.4127
aF97.4099
aF97.0275
aF97.4252
aF97.3261
aF97.3392
aF97.3701
aF97.6732
aF97.7432
aF97.3822
aF97.2275
aF97.343
aF97.7222
aF97.4242
aF97.0178
aF97.5329
aF97.2531
aF97.8007
aF97.3086
aF97.5116
aF97.7532
aF97.4486
aF97.6083
aF97.2022
aF97.4586
aF97.6543
aF97.3573
aF97.6514
aF97.1149
aF97.4303
aF97.559
aF97.2103
aF97.4643
aF97.661
aF97.2496
aF97.1827
aF97.1235
aF97.5565
aF97.2022
aF97.5083
aF97.4473
aF97.6712
aF97.4286
aF97.5028
aF97.4012
aF97.469
aF97.4703
aF97.3137
aF97.6175
aF97.3593
aF97.2692
aF97.293
aF97.2135
aF97.4124
aF97.443
aF97.4723
aF97.3041
aF97.4939
aF97.2754
aF97.3706
aF97.4682
aF97.7012
aF97.4764
aF97.3795
aF97.1705
aF97.4586
aF97.332
aF97.3309
aF97.291
aF97.2263
aF97.3897
atp91
tp92
Rp93
aatp94
Rp95
sS'loaded_snapshot_epoch'
p96
NsS'pretrained_model'
p97
S'/projeto/programs/digits/jobs/20170710-193454-3eed/snapshot_iter_2520.caffemodel'
p98
sS'deploy_file'
p99
S'deploy.prototxt'
p100
sS'current_iteration'
p101
I6300
sS'crop_size'
p102
NsS'rms_decay'
p103
F0.99
sS'train_epochs'
p104
I100
sS'network'
p105
ccaffe_pb2
NetParameter
p106
(tRp107
(dp108
S'serialized'
p109
S'\n\tDetectNet\xa2\x06#\n\ntrain_data\x12\x04Data"\x04dataB\x02\x08\x00\xda\x06\x04 \n@\x01\xa2\x06%\n\x0btrain_label\x12\x04Data"\x05labelB\x02\x08\x00\xda\x06\x04 \n@\x01\xa2\x06&\n\x08val_data\x12\x04Data"\x04dataB\x07\x08\x01"\x03val\xda\x06\x04 \x06@\x01\xa2\x06(\n\tval_label\x12\x04Data"\x05labelB\x07\x08\x01"\x03val\xda\x06\x04 \x06@\x01\xa2\x060\n\x0bdeploy_data\x12\x05Input"\x04dataB\x07\x08\x01*\x03val\xfa\x08\n\n\x08\n\x06\x01\x03\x80\x05\x80\x08\xa2\x06\xc8\x01\n\x0ftrain_transform\x12\x17DetectNetTransformation\x1a\x04data\x1a\x05label"\x10transformed_data"\x11transformed_labelB\x02\x08\x00\xa2\x06\x05-\x00\x00\xfeB\x8a\xa9\x03\x1d\x08\x10\x15\xcd\xcc\xcc>\x18\x01(\x148\x00@\x80\x08H\x80\x05X\x01`\x01j\x04\x08\x01\x10\x00\x92\xa9\x03;\r\x00\x00\x80?\x10 \x18 %\xcd\xcc\xcc>-\xcd\xccL?5\x9a\x99\x99?=\x00\x00\x00?E\x00\x00\x00\x00M\x00\x00\xa0@U\xcd\xccL?]\x00\x00\xf0Ae\xcd\xccL?m\xcd\xccL?\xa2\x06\x8c\x01\n\rval_transform\x12\x17DetectNetTransformation\x1a\x04data\x1a\x05label"\x10transformed_data"\x11transformed_labelB\x07\x08\x01"\x03val\xa2\x06\x05-\x00\x00\xfeB\x8a\xa9\x03\x1d\x08\x10\x15\xcd\xcc\xcc>\x18\x01(\x148\x00@\x80\x08H\x80\x05X\x01`\x00j\x04\x08\x01\x10\x00\xa2\x06B\n\x10deploy_transform\x12\x05Power\x1a\x04data"\x10transformed_dataB\x07\x08\x01*\x03val\xd2\x07\x05\x1d\x00\x00\xfe\xc2\xa2\x06\x86\x01\n\x0bslice-label\x12\x05Slice\x1a\x11transformed_label"\x10foreground-label"\nbbox-label"\nsize-label"\tobj-label"\x0ecoverage-labelB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x07\n\x08\x01\x10\x01\x10\x05\x10\x07\x10\x08\xa2\x06\x82\x01\n\x0ecoverage-block\x12\x06Concat\x1a\x10foreground-label\x1a\x10foreground-label\x1a\x10foreground-label\x1a\x10foreground-label"\x0ecoverage-blockB\x02\x08\x00B\x07\x08\x01"\x03val\xc2\x06\x02\x08\x01\xa2\x06J\n\nsize-block\x12\x06Concat\x1a\nsize-label\x1a\nsize-label"\nsize-blockB\x02\x08\x00B\x07\x08\x01"\x03val\xc2\x06\x02\x08\x01\xa2\x06\\\n\tobj-block\x12\x06Concat\x1a\tobj-label\x1a\tobj-label\x1a\tobj-label\x1a\tobj-label"\tobj-blockB\x02\x08\x00B\x07\x08\x01"\x03val\xc2\x06\x02\x08\x01\xa2\x06S\n\rbb-label-norm\x12\x07Eltwise\x1a\nbbox-label\x1a\nsize-block"\x0fbbox-label-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06Y\n\x0bbb-obj-norm\x12\x07Eltwise\x1a\x0fbbox-label-norm\x1a\tobj-block"\x13bbox-obj-label-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06~\n\x0cconv1/7x7_s2\x12\x0bConvolution\x1a\x10transformed_data"\x0cconv1/7x7_s22\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06(\x08@\x18\x03 \x070\x02:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x062\n\x0econv1/relu_7x7\x12\x04ReLU\x1a\x0cconv1/7x7_s2"\x0cconv1/7x7_s2\xa2\x06<\n\x0cpool1/3x3_s2\x12\x07Pooling\x1a\x0cconv1/7x7_s2"\x0cpool1/3x3_s2\xca\x07\x06\x08\x00\x10\x03\x18\x02\xa2\x06<\n\x0bpool1/norm1\x12\x03LRN\x1a\x0cpool1/3x3_s2"\x0bpool1/norm1\xb2\x07\x0c\x08\x05\x15\x17\xb7\xd18\x1d\x00\x00@?\xa2\x06}\n\x10conv2/3x3_reduce\x12\x0bConvolution\x1a\x0bpool1/norm1"\x10conv2/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15conv2/relu_3x3_reduce\x12\x04ReLU\x1a\x10conv2/3x3_reduce"\x10conv2/3x3_reduce\xa2\x06w\n\tconv2/3x3\x12\x0bConvolution\x1a\x10conv2/3x3_reduce"\tconv2/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06,\n\x0econv2/relu_3x3\x12\x04ReLU\x1a\tconv2/3x3"\tconv2/3x3\xa2\x069\n\x0bconv2/norm2\x12\x03LRN\x1a\tconv2/3x3"\x0bconv2/norm2\xb2\x07\x0c\x08\x05\x15\x17\xb7\xd18\x1d\x00\x00@?\xa2\x06;\n\x0cpool2/3x3_s2\x12\x07Pooling\x1a\x0bconv2/norm2"\x0cpool2/3x3_s2\xca\x07\x06\x08\x00\x10\x03\x18\x02\xa2\x06~\n\x10inception_3a/1x1\x12\x0bConvolution\x1a\x0cpool2/3x3_s2"\x10inception_3a/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3a/relu_1x1\x12\x04ReLU\x1a\x10inception_3a/1x1"\x10inception_3a/1x1\xa2\x06\x8c\x01\n\x17inception_3a/3x3_reduce\x12\x0bConvolution\x1a\x0cpool2/3x3_s2"\x17inception_3a/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08` \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3a/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_3a/3x3_reduce"\x17inception_3a/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_3a/3x3\x12\x0bConvolution\x1a\x17inception_3a/3x3_reduce"\x10inception_3a/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3a/relu_3x3\x12\x04ReLU\x1a\x10inception_3a/3x3"\x10inception_3a/3x3\xa2\x06\x8c\x01\n\x17inception_3a/5x5_reduce\x12\x0bConvolution\x1a\x0cpool2/3x3_s2"\x17inception_3a/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x10 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3a/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_3a/5x5_reduce"\x17inception_3a/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_3a/5x5\x12\x0bConvolution\x1a\x17inception_3a/5x5_reduce"\x10inception_3a/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08 \x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3a/relu_5x5\x12\x04ReLU\x1a\x10inception_3a/5x5"\x10inception_3a/5x5\xa2\x06H\n\x11inception_3a/pool\x12\x07Pooling\x1a\x0cpool2/3x3_s2"\x11inception_3a/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_3a/pool_proj\x12\x0bConvolution\x1a\x11inception_3a/pool"\x16inception_3a/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_3a/relu_pool_proj\x12\x04ReLU\x1a\x16inception_3a/pool_proj"\x16inception_3a/pool_proj\xa2\x06\x80\x01\n\x13inception_3a/output\x12\x06Concat\x1a\x10inception_3a/1x1\x1a\x10inception_3a/3x3\x1a\x10inception_3a/5x5\x1a\x16inception_3a/pool_proj"\x13inception_3a/output\xa2\x06\x86\x01\n\x10inception_3b/1x1\x12\x0bConvolution\x1a\x13inception_3a/output"\x10inception_3b/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3b/relu_1x1\x12\x04ReLU\x1a\x10inception_3b/1x1"\x10inception_3b/1x1\xa2\x06\x94\x01\n\x17inception_3b/3x3_reduce\x12\x0bConvolution\x1a\x13inception_3a/output"\x17inception_3b/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3b/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_3b/3x3_reduce"\x17inception_3b/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_3b/3x3\x12\x0bConvolution\x1a\x17inception_3b/3x3_reduce"\x10inception_3b/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3b/relu_3x3\x12\x04ReLU\x1a\x10inception_3b/3x3"\x10inception_3b/3x3\xa2\x06\x93\x01\n\x17inception_3b/5x5_reduce\x12\x0bConvolution\x1a\x13inception_3a/output"\x17inception_3b/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_3b/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_3b/5x5_reduce"\x17inception_3b/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_3b/5x5\x12\x0bConvolution\x1a\x17inception_3b/5x5_reduce"\x10inception_3b/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08`\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_3b/relu_5x5\x12\x04ReLU\x1a\x10inception_3b/5x5"\x10inception_3b/5x5\xa2\x06O\n\x11inception_3b/pool\x12\x07Pooling\x1a\x13inception_3a/output"\x11inception_3b/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_3b/pool_proj\x12\x0bConvolution\x1a\x11inception_3b/pool"\x16inception_3b/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_3b/relu_pool_proj\x12\x04ReLU\x1a\x16inception_3b/pool_proj"\x16inception_3b/pool_proj\xa2\x06\x80\x01\n\x13inception_3b/output\x12\x06Concat\x1a\x10inception_3b/1x1\x1a\x10inception_3b/3x3\x1a\x10inception_3b/5x5\x1a\x16inception_3b/pool_proj"\x13inception_3b/output\xa2\x06C\n\x0cpool3/3x3_s2\x12\x07Pooling\x1a\x13inception_3b/output"\x0cpool3/3x3_s2\xca\x07\x06\x08\x00\x10\x03\x18\x02\xa2\x06\x7f\n\x10inception_4a/1x1\x12\x0bConvolution\x1a\x0cpool3/3x3_s2"\x10inception_4a/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xc0\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4a/relu_1x1\x12\x04ReLU\x1a\x10inception_4a/1x1"\x10inception_4a/1x1\xa2\x06\x8c\x01\n\x17inception_4a/3x3_reduce\x12\x0bConvolution\x1a\x0cpool3/3x3_s2"\x17inception_4a/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08` \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4a/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4a/3x3_reduce"\x17inception_4a/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4a/3x3\x12\x0bConvolution\x1a\x17inception_4a/3x3_reduce"\x10inception_4a/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xd0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4a/relu_3x3\x12\x04ReLU\x1a\x10inception_4a/3x3"\x10inception_4a/3x3\xa2\x06\x8c\x01\n\x17inception_4a/5x5_reduce\x12\x0bConvolution\x1a\x0cpool3/3x3_s2"\x17inception_4a/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x10 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4a/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4a/5x5_reduce"\x17inception_4a/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4a/5x5\x12\x0bConvolution\x1a\x17inception_4a/5x5_reduce"\x10inception_4a/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x080\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4a/relu_5x5\x12\x04ReLU\x1a\x10inception_4a/5x5"\x10inception_4a/5x5\xa2\x06H\n\x11inception_4a/pool\x12\x07Pooling\x1a\x0cpool3/3x3_s2"\x11inception_4a/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4a/pool_proj\x12\x0bConvolution\x1a\x11inception_4a/pool"\x16inception_4a/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4a/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4a/pool_proj"\x16inception_4a/pool_proj\xa2\x06\x80\x01\n\x13inception_4a/output\x12\x06Concat\x1a\x10inception_4a/1x1\x1a\x10inception_4a/3x3\x1a\x10inception_4a/5x5\x1a\x16inception_4a/pool_proj"\x13inception_4a/output\xa2\x06\x86\x01\n\x10inception_4b/1x1\x12\x0bConvolution\x1a\x13inception_4a/output"\x10inception_4b/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xa0\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4b/relu_1x1\x12\x04ReLU\x1a\x10inception_4b/1x1"\x10inception_4b/1x1\xa2\x06\x93\x01\n\x17inception_4b/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4a/output"\x17inception_4b/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08p \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4b/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4b/3x3_reduce"\x17inception_4b/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4b/3x3\x12\x0bConvolution\x1a\x17inception_4b/3x3_reduce"\x10inception_4b/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xe0\x01\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4b/relu_3x3\x12\x04ReLU\x1a\x10inception_4b/3x3"\x10inception_4b/3x3\xa2\x06\x93\x01\n\x17inception_4b/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4a/output"\x17inception_4b/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x18 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4b/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4b/5x5_reduce"\x17inception_4b/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4b/5x5\x12\x0bConvolution\x1a\x17inception_4b/5x5_reduce"\x10inception_4b/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08@\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4b/relu_5x5\x12\x04ReLU\x1a\x10inception_4b/5x5"\x10inception_4b/5x5\xa2\x06O\n\x11inception_4b/pool\x12\x07Pooling\x1a\x13inception_4a/output"\x11inception_4b/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4b/pool_proj\x12\x0bConvolution\x1a\x11inception_4b/pool"\x16inception_4b/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4b/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4b/pool_proj"\x16inception_4b/pool_proj\xa2\x06\x80\x01\n\x13inception_4b/output\x12\x06Concat\x1a\x10inception_4b/1x1\x1a\x10inception_4b/3x3\x1a\x10inception_4b/5x5\x1a\x16inception_4b/pool_proj"\x13inception_4b/output\xa2\x06\x86\x01\n\x10inception_4c/1x1\x12\x0bConvolution\x1a\x13inception_4b/output"\x10inception_4c/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4c/relu_1x1\x12\x04ReLU\x1a\x10inception_4c/1x1"\x10inception_4c/1x1\xa2\x06\x94\x01\n\x17inception_4c/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4b/output"\x17inception_4c/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4c/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4c/3x3_reduce"\x17inception_4c/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4c/3x3\x12\x0bConvolution\x1a\x17inception_4c/3x3_reduce"\x10inception_4c/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x02\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4c/relu_3x3\x12\x04ReLU\x1a\x10inception_4c/3x3"\x10inception_4c/3x3\xa2\x06\x93\x01\n\x17inception_4c/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4b/output"\x17inception_4c/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x18 \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4c/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4c/5x5_reduce"\x17inception_4c/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4c/5x5\x12\x0bConvolution\x1a\x17inception_4c/5x5_reduce"\x10inception_4c/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08@\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4c/relu_5x5\x12\x04ReLU\x1a\x10inception_4c/5x5"\x10inception_4c/5x5\xa2\x06O\n\x11inception_4c/pool\x12\x07Pooling\x1a\x13inception_4b/output"\x11inception_4c/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4c/pool_proj\x12\x0bConvolution\x1a\x11inception_4c/pool"\x16inception_4c/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4c/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4c/pool_proj"\x16inception_4c/pool_proj\xa2\x06\x80\x01\n\x13inception_4c/output\x12\x06Concat\x1a\x10inception_4c/1x1\x1a\x10inception_4c/3x3\x1a\x10inception_4c/5x5\x1a\x16inception_4c/pool_proj"\x13inception_4c/output\xa2\x06\x85\x01\n\x10inception_4d/1x1\x12\x0bConvolution\x1a\x13inception_4c/output"\x10inception_4d/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08p \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4d/relu_1x1\x12\x04ReLU\x1a\x10inception_4d/1x1"\x10inception_4d/1x1\xa2\x06\x94\x01\n\x17inception_4d/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4c/output"\x17inception_4d/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x90\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4d/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4d/3x3_reduce"\x17inception_4d/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4d/3x3\x12\x0bConvolution\x1a\x17inception_4d/3x3_reduce"\x10inception_4d/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xa0\x02\x18\x01 \x03:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4d/relu_3x3\x12\x04ReLU\x1a\x10inception_4d/3x3"\x10inception_4d/3x3\xa2\x06\x93\x01\n\x17inception_4d/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4c/output"\x17inception_4d/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4d/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4d/5x5_reduce"\x17inception_4d/5x5_reduce\xa2\x06\x8b\x01\n\x10inception_4d/5x5\x12\x0bConvolution\x1a\x17inception_4d/5x5_reduce"\x10inception_4d/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06&\x08@\x18\x02 \x05:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4d/relu_5x5\x12\x04ReLU\x1a\x10inception_4d/5x5"\x10inception_4d/5x5\xa2\x06O\n\x11inception_4d/pool\x12\x07Pooling\x1a\x13inception_4c/output"\x11inception_4d/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x8f\x01\n\x16inception_4d/pool_proj\x12\x0bConvolution\x1a\x11inception_4d/pool"\x16inception_4d/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08@ \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4d/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4d/pool_proj"\x16inception_4d/pool_proj\xa2\x06\x80\x01\n\x13inception_4d/output\x12\x06Concat\x1a\x10inception_4d/1x1\x1a\x10inception_4d/3x3\x1a\x10inception_4d/5x5\x1a\x16inception_4d/pool_proj"\x13inception_4d/output\xa2\x06\x86\x01\n\x10inception_4e/1x1\x12\x0bConvolution\x1a\x13inception_4d/output"\x10inception_4e/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x02 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4e/relu_1x1\x12\x04ReLU\x1a\x10inception_4e/1x1"\x10inception_4e/1x1\xa2\x06\x94\x01\n\x17inception_4e/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4d/output"\x17inception_4e/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xa0\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4e/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_4e/3x3_reduce"\x17inception_4e/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_4e/3x3\x12\x0bConvolution\x1a\x17inception_4e/3x3_reduce"\x10inception_4e/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x02\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4e/relu_3x3\x12\x04ReLU\x1a\x10inception_4e/3x3"\x10inception_4e/3x3\xa2\x06\x93\x01\n\x17inception_4e/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4d/output"\x17inception_4e/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_4e/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_4e/5x5_reduce"\x17inception_4e/5x5_reduce\xa2\x06\x8c\x01\n\x10inception_4e/5x5\x12\x0bConvolution\x1a\x17inception_4e/5x5_reduce"\x10inception_4e/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_4e/relu_5x5\x12\x04ReLU\x1a\x10inception_4e/5x5"\x10inception_4e/5x5\xa2\x06O\n\x11inception_4e/pool\x12\x07Pooling\x1a\x13inception_4d/output"\x11inception_4e/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x90\x01\n\x16inception_4e/pool_proj\x12\x0bConvolution\x1a\x11inception_4e/pool"\x16inception_4e/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_4e/relu_pool_proj\x12\x04ReLU\x1a\x16inception_4e/pool_proj"\x16inception_4e/pool_proj\xa2\x06\x80\x01\n\x13inception_4e/output\x12\x06Concat\x1a\x10inception_4e/1x1\x1a\x10inception_4e/3x3\x1a\x10inception_4e/5x5\x1a\x16inception_4e/pool_proj"\x13inception_4e/output\xa2\x06\x86\x01\n\x10inception_5a/1x1\x12\x0bConvolution\x1a\x13inception_4e/output"\x10inception_5a/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x02 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5a/relu_1x1\x12\x04ReLU\x1a\x10inception_5a/1x1"\x10inception_5a/1x1\xa2\x06\x94\x01\n\x17inception_5a/3x3_reduce\x12\x0bConvolution\x1a\x13inception_4e/output"\x17inception_5a/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\xa0\x01 \x01:\r\n\x06xavier5\xecQ\xb8=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5a/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_5a/3x3_reduce"\x17inception_5a/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_5a/3x3\x12\x0bConvolution\x1a\x17inception_5a/3x3_reduce"\x10inception_5a/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\xc0\x02\x18\x01 \x03:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5a/relu_3x3\x12\x04ReLU\x1a\x10inception_5a/3x3"\x10inception_5a/3x3\xa2\x06\x93\x01\n\x17inception_5a/5x5_reduce\x12\x0bConvolution\x1a\x13inception_4e/output"\x17inception_5a/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08  \x01:\r\n\x06xavier5\xcd\xccL>B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5a/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_5a/5x5_reduce"\x17inception_5a/5x5_reduce\xa2\x06\x8c\x01\n\x10inception_5a/5x5\x12\x0bConvolution\x1a\x17inception_5a/5x5_reduce"\x10inception_5a/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x02 \x05:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5a/relu_5x5\x12\x04ReLU\x1a\x10inception_5a/5x5"\x10inception_5a/5x5\xa2\x06O\n\x11inception_5a/pool\x12\x07Pooling\x1a\x13inception_4e/output"\x11inception_5a/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x90\x01\n\x16inception_5a/pool_proj\x12\x0bConvolution\x1a\x11inception_5a/pool"\x16inception_5a/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_5a/relu_pool_proj\x12\x04ReLU\x1a\x16inception_5a/pool_proj"\x16inception_5a/pool_proj\xa2\x06\x80\x01\n\x13inception_5a/output\x12\x06Concat\x1a\x10inception_5a/1x1\x1a\x10inception_5a/3x3\x1a\x10inception_5a/5x5\x1a\x16inception_5a/pool_proj"\x13inception_5a/output\xa2\x06\x86\x01\n\x10inception_5b/1x1\x12\x0bConvolution\x1a\x13inception_5a/output"\x10inception_5b/1x12\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x03 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5b/relu_1x1\x12\x04ReLU\x1a\x10inception_5b/1x1"\x10inception_5b/1x1\xa2\x06\x94\x01\n\x17inception_5b/3x3_reduce\x12\x0bConvolution\x1a\x13inception_5a/output"\x17inception_5b/3x3_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x80?%\x00\x00\x00\x00\xd2\x06%\x08\xc0\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5b/relu_3x3_reduce\x12\x04ReLU\x1a\x17inception_5b/3x3_reduce"\x17inception_5b/3x3_reduce\xa2\x06\x8c\x01\n\x10inception_5b/3x3\x12\x0bConvolution\x1a\x17inception_5b/3x3_reduce"\x10inception_5b/3x32\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x03\x18\x01 \x03:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5b/relu_3x3\x12\x04ReLU\x1a\x10inception_5b/3x3"\x10inception_5b/3x3\xa2\x06\x93\x01\n\x17inception_5b/5x5_reduce\x12\x0bConvolution\x1a\x13inception_5a/output"\x17inception_5b/5x5_reduce2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x080 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06V\n\x1cinception_5b/relu_5x5_reduce\x12\x04ReLU\x1a\x17inception_5b/5x5_reduce"\x17inception_5b/5x5_reduce\xa2\x06\x8c\x01\n\x10inception_5b/5x5\x12\x0bConvolution\x1a\x17inception_5b/5x5_reduce"\x10inception_5b/5x52\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06\'\x08\x80\x01\x18\x02 \x05:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06A\n\x15inception_5b/relu_5x5\x12\x04ReLU\x1a\x10inception_5b/5x5"\x10inception_5b/5x5\xa2\x06O\n\x11inception_5b/pool\x12\x07Pooling\x1a\x13inception_5a/output"\x11inception_5b/pool\xca\x07\x08\x08\x00\x10\x03\x18\x01 \x01\xa2\x06\x90\x01\n\x16inception_5b/pool_proj\x12\x0bConvolution\x1a\x11inception_5b/pool"\x16inception_5b/pool_proj2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06%\x08\x80\x01 \x01:\r\n\x06xavier5\xcd\xcc\xcc=B\x0f\n\x08constant\x15\xcd\xccL>\xa2\x06S\n\x1binception_5b/relu_pool_proj\x12\x04ReLU\x1a\x16inception_5b/pool_proj"\x16inception_5b/pool_proj\xa2\x06\x80\x01\n\x13inception_5b/output\x12\x06Concat\x1a\x10inception_5b/1x1\x1a\x10inception_5b/3x3\x1a\x10inception_5b/5x5\x1a\x16inception_5b/pool_proj"\x13inception_5b/output\xa2\x06D\n\rpool5/drop_s1\x12\x07Dropout\x1a\x13inception_5b/output"\rpool5/drop_s1\xe2\x06\x05\r\xcd\xcc\xcc>\xa2\x06{\n\x0ecvg/classifier\x12\x0bConvolution\x1a\rpool5/drop_s1"\x0ecvg/classifier2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x01 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\x00\x00\x00\x00\xa2\x061\n\x0ccoverage/sig\x12\x07Sigmoid\x1a\x0ecvg/classifier"\x08coverage\xa2\x06s\n\x0ebbox/regressor\x12\x0bConvolution\x1a\rpool5/drop_s1"\x06bboxes2\n\x1d\x00\x00\x80?%\x00\x00\x80?2\n\x1d\x00\x00\x00@%\x00\x00\x00\x00\xd2\x06$\x08\x04 \x01:\r\n\x06xavier5\x8f\xc2\xf5<B\x0f\n\x08constant\x15\x00\x00\x00\x00\xa2\x06M\n\tbbox_mask\x12\x07Eltwise\x1a\x06bboxes\x1a\x0ecoverage-block"\rbboxes-maskedB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06U\n\tbbox-norm\x12\x07Eltwise\x1a\rbboxes-masked\x1a\nsize-block"\x12bboxes-masked-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06a\n\rbbox-obj-norm\x12\x07Eltwise\x1a\x12bboxes-masked-norm\x1a\tobj-block"\x16bboxes-obj-masked-normB\x02\x08\x00B\x07\x08\x01"\x03val\xf2\x06\x02\x08\x00\xa2\x06]\n\tbbox_loss\x12\x06L1Loss\x1a\x16bboxes-obj-masked-norm\x1a\x13bbox-obj-label-norm"\tloss_bbox-\x00\x00\x00@B\x02\x08\x00B\x07\x08\x01"\x03val\xa2\x06T\n\rcoverage_loss\x12\rEuclideanLoss\x1a\x08coverage\x1a\x0ecoverage-label"\rloss_coverageB\x02\x08\x00B\x07\x08\x01"\x03val\xa2\x06\x8f\x01\n\x07cluster\x12\x06Python\x1a\x08coverage\x1a\x06bboxes"\tbbox-listB\x02\x08\x01\x92\x08Z\n!caffe.layers.detectnet.clustering\x12\x11ClusterDetections\x1a"1024, 618, 16, 0.6, 3, 0.02, 22, 1\xa2\x06\x96\x01\n\ncluster_gt\x12\x06Python\x1a\x0ecoverage-label\x1a\nbbox-label"\x0fbbox-list-labelB\x07\x08\x01"\x03val\x92\x08I\n!caffe.layers.detectnet.clustering\x12\x12ClusterGroundtruth\x1a\x101024, 618, 16, 1\xa2\x06z\n\x05score\x12\x06Python\x1a\x0fbbox-list-label\x1a\tbbox-list"\x10bbox-list-scoredB\x07\x08\x01"\x03val\x92\x081\n\x1ecaffe.layers.detectnet.mean_ap\x12\x0fScoreDetections\xa2\x06w\n\x03mAP\x12\x06Python\x1a\x10bbox-list-scored"\x03mAP"\tprecision"\x06recallB\x07\x08\x01"\x03val\x92\x084\n\x1ecaffe.layers.detectnet.mean_ap\x12\x03mAP\x1a\r1024, 618, 16'
p110
sbsS'use_mean'
p111
Vnone
p112
sS'model_file'
p113
S'original.prototxt'
p114
sS'job_dir'
p115
S'/projeto/programs/digits/jobs/20170710-221135-1561'
p116
sS'receiving_val_output'
p117
I00
sS'parents'
p118
NsS'caffe_flavor'
p119
S'NVIDIA'
p120
sS'progress'
p121
F1.0
sS'lr_policy'
p122
(dp123
S'policy'
p124
Vfixed
p125
ssS'gpu_count'
p126
NsS'receiving_train_output'
p127
I00
sS'val_interval'
p128
F1.0
sS'current_epoch'
p129
F100.0
sS'random_seed'
p130
NsS'saving_snapshot'
p131
I00
sS'pickver_task_caffe_train'
p132
I5
sg44
F0.0001
sS'loaded_snapshot_file'
p133
NsS'batch_size'
p134
I4
sS'digits_version'
p135
S'5.0.0'
p136
sS'job'
p137
g4
sS'pickver_task_train'
p138
I2
sS'current_resources'
p139
NsS'pickver_task'
p140
I1
sS'job_id'
p141
S'20170710-221135-1561'
p142
sS'exception'
p143
NsS'data_aug'
p144
(dp145
S'hsv_h'
p146
F0.02
sS'hsv_use'
p147
I00
sS'flip'
p148
Vnone
p149
sS'quad_rot'
p150
Vnone
p151
sS'scale'
p152
F0.0
sS'noise'
p153
F0.0
sS'hsv_s'
p154
F0.04
sS'rot'
p155
I0
sS'hsv_v'
p156
F0.06
ssS'selected_gpus'
p157
(lp158
S'0'
p159
aS'1'
p160
aS'2'
p161
aS'3'
p162
asS'solver'
p163
ccaffe_pb2
SolverParameter
p164
(tRp165
(dp166
g109
S'\x18\xfa\x01 ?-\x17\xb7\xd180\x078\x9c1B\x05fixed]fff?e\xbd7\x865p?z\x08snapshot\x88\x01\x01\xc2\x01\x12train_val.prototxt\xf0\x01\x05\xa0\x02\x04'
p167
sbsS'snapshot_prefix'
p168
S'snapshot'
p169
sS'traceback'
p170
NsS'framework_id'
p171
S'caffe'
p172
sS'status_history'
p173
(lp174
((idigits.status
Status
p175
S'I'
p177
bF1499724695.267241
tp178
a((idigits.status
Status
p179
S'R'
p180
bF1499724696.925749
tp181
a((idigits.status
Status
p182
S'D'
p183
bF1499732872.20809
tp184
asS'last_train_update'
p185
F1499732828.150688
sS'train_val_file'
p186
S'train_val.prototxt'
p187
sS'image_mean'
p188
NsS'batch_accumulation'
p189
I4
sS'caffe_version'
p190
S'0.15.13'
p191
sS'log_file'
p192
S'caffe_output.log'
p193
sS'solver_file'
p194
S'solver.prototxt'
p195
sS'solver_type'
p196
VADAM
p197
sbasS'pickver_job_model_image_generic'
p198
I1
sS'pickver_job'
p199
I2
sS'dataset_id'
p200
S'20170710-193022-dce9'
p201
sS'pickver_job_model_image'
p202
I1
sg143
NsS'group'
p203
Vmccdetection
p204
sS'persistent'
p205
I01
sS'_name'
p206
Vmcc_detector_d2_ep30_lr0001_mccmultsyth_pretrain_v003
p207
sS'form_data'
p208
(dp209
S'form.standard_networks.data'
p210
VNone
p211
sS'form.learning_rate.data'
p212
(lp213
F0.0001
asS'form.model_name.data'
p214
g207
sS'form.aug_flip.data'
p215
g149
sS'form.select_gpus.data'
p216
(lp217
V0
p218
aV1
p219
aV2
p220
aV3
p221
asS'form.framework.data'
p222
Vcaffe
p223
sS'form.dataset.data'
p224
V20170710-193022-dce9
p225
sS'form.python_layer_from_client.data'
p226
I00
sS'form.lr_multistep_values.data'
p227
V50,85
p228
sS'form.aug_hsv_s.data'
p229
F0.04
sS'form.lr_sigmoid_gamma.data'
p230
F0.1
sS'form.lr_policy.data'
p231
g125
sS'form.random_seed.data'
p232
NsS'form.previous_networks.data'
p233
V20170710-193454-3eed
p234
sS'form.aug_quad_rot.data'
p235
g151
sS'form.rms_decay.data'
p236
F0.99
sS'form.method.data'
p237
Vprevious
p238
sS'form.lr_sigmoid_step.data'
p239
F50.0
sS'form.crop_size.data'
p240
NsS'form.select_gpu_count.data'
p241
NsS'form.python_layer_server_file.data'
p242
V
p243
sS'form.pretrained_networks.data'
p244
VNone
p245
sS'form.solver_type.data'
p246
g197
sS'form.aug_noise.data'
p247
F0.0
sS'form.select_gpus_list.data'
p248
g243
sS'form.shuffle.data'
p249
I01
sS'form.aug_scale.data'
p250
F0.0
sS'form.batch_accumulation.data'
p251
I4
sS'form.custom_network.data'
p252
V# DetectNet network\u000a\u000a# Data/Input layers\u000aname: "DetectNet"\u000alayer {\u000a  name: "train_data"\u000a  type: "Data"\u000a  top: "data"\u000a  data_param {\u000a    backend: LMDB\u000a    batch_size: 10\u000a  }\u000a  include: { phase: TRAIN }\u000a}\u000alayer {\u000a  name: "train_label"\u000a  type: "Data"\u000a  top: "label"\u000a  data_param {\u000a    backend: LMDB\u000a    batch_size: 10\u000a  }\u000a  include: { phase: TRAIN }\u000a}\u000alayer {\u000a  name: "val_data"\u000a  type: "Data"\u000a  top: "data"\u000a  data_param {\u000a    backend: LMDB\u000a    batch_size: 6\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "val_label"\u000a  type: "Data"\u000a  top: "label"\u000a  data_param {\u000a    backend: LMDB\u000a    batch_size: 6\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "deploy_data"\u000a  type: "Input"\u000a  top: "data"\u000a  input_param {\u000a    shape {\u000a      dim: 1\u000a      dim: 3\u000a      dim: 640\u000a      dim: 1024\u000a    }\u000a  }\u000a  include: { phase: TEST not_stage: "val" }\u000a}\u000a\u000a# Data transformation layers\u000alayer {\u000a  name: "train_transform"\u000a  type: "DetectNetTransformation"\u000a  bottom: "data"\u000a  bottom: "label"\u000a  top: "transformed_data"\u000a  top: "transformed_label"\u000a  detectnet_groundtruth_param: {\u000a    stride: 16\u000a    scale_cvg: 0.4\u000a    gridbox_type: GRIDBOX_MIN\u000a    coverage_type: RECTANGULAR\u000a    min_cvg_len: 20\u000a    obj_norm: true\u000a    image_size_x: 1024\u000a    image_size_y: 640\u000a    crop_bboxes: true\u000a    object_class: { src: 1 dst: 0} # obj class 1 -> cvg index 0\u000a  }\u000a   detectnet_augmentation_param: {\u000a    crop_prob: 1\u000a    shift_x: 32\u000a    shift_y: 32\u000a    flip_prob: 0.5\u000a    rotation_prob: 0\u000a    max_rotate_degree: 5\u000a    scale_prob: 0.4\u000a    scale_min: 0.8\u000a    scale_max: 1.2\u000a    hue_rotation_prob: 0.8\u000a    hue_rotation: 30\u000a    desaturation_prob: 0.8\u000a    desaturation_max: 0.8\u000a  }\u000a  transform_param: {\u000a    mean_value: 127\u000a  }\u000a  include: { phase: TRAIN }\u000a}\u000alayer {\u000a  name: "val_transform"\u000a  type: "DetectNetTransformation"\u000a  bottom: "data"\u000a  bottom: "label"\u000a  top: "transformed_data"\u000a  top: "transformed_label"\u000a  detectnet_groundtruth_param: {\u000a    stride: 16\u000a    scale_cvg: 0.4\u000a    gridbox_type: GRIDBOX_MIN\u000a    coverage_type: RECTANGULAR\u000a    min_cvg_len: 20\u000a    obj_norm: true\u000a    image_size_x: 1024\u000a    image_size_y: 640\u000a    crop_bboxes: false\u000a    object_class: { src: 1 dst: 0} # obj class 1 -> cvg index 0\u000a  }\u000a  transform_param: {\u000a    mean_value: 127\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "deploy_transform"\u000a  type: "Power"\u000a  bottom: "data"\u000a  top: "transformed_data"\u000a  power_param {\u000a    shift: -127\u000a  }\u000a  include: { phase: TEST not_stage: "val" }\u000a}\u000a\u000a# Label conversion layers\u000alayer {\u000a  name: "slice-label"\u000a  type: "Slice"\u000a  bottom: "transformed_label"\u000a  top: "foreground-label"\u000a  top: "bbox-label"\u000a  top: "size-label"\u000a  top: "obj-label"\u000a  top: "coverage-label"\u000a  slice_param {\u000a    slice_dim: 1\u000a    slice_point: 1\u000a    slice_point: 5\u000a    slice_point: 7\u000a    slice_point: 8\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "coverage-block"\u000a  type: "Concat"\u000a  bottom: "foreground-label"\u000a  bottom: "foreground-label"\u000a  bottom: "foreground-label"\u000a  bottom: "foreground-label"\u000a  top: "coverage-block"\u000a  concat_param {\u000a    concat_dim: 1\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "size-block"\u000a  type: "Concat"\u000a  bottom: "size-label"\u000a  bottom: "size-label"\u000a  top: "size-block"\u000a  concat_param {\u000a    concat_dim: 1\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "obj-block"\u000a  type: "Concat"\u000a  bottom: "obj-label"\u000a  bottom: "obj-label"\u000a  bottom: "obj-label"\u000a  bottom: "obj-label"\u000a  top: "obj-block"\u000a  concat_param {\u000a    concat_dim: 1\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bb-label-norm"\u000a  type: "Eltwise"\u000a  bottom: "bbox-label"\u000a  bottom: "size-block"\u000a  top: "bbox-label-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bb-obj-norm"\u000a  type: "Eltwise"\u000a  bottom: "bbox-label-norm"\u000a  bottom: "obj-block"\u000a  top: "bbox-obj-label-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000a\u000a######################################################################\u000a# Start of convolutional network\u000a######################################################################\u000a\u000alayer {\u000a  name: "conv1/7x7_s2"\u000a  type: "Convolution"\u000a  bottom: "transformed_data"\u000a  top: "conv1/7x7_s2"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 3\u000a    kernel_size: 7\u000a    stride: 2\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv1/relu_7x7"\u000a  type: "ReLU"\u000a  bottom: "conv1/7x7_s2"\u000a  top: "conv1/7x7_s2"\u000a}\u000a\u000alayer {\u000a  name: "pool1/3x3_s2"\u000a  type: "Pooling"\u000a  bottom: "conv1/7x7_s2"\u000a  top: "pool1/3x3_s2"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 2\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "pool1/norm1"\u000a  type: "LRN"\u000a  bottom: "pool1/3x3_s2"\u000a  top: "pool1/norm1"\u000a  lrn_param {\u000a    local_size: 5\u000a    alpha: 0.0001\u000a    beta: 0.75\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv2/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool1/norm1"\u000a  top: "conv2/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv2/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "conv2/3x3_reduce"\u000a  top: "conv2/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "conv2/3x3"\u000a  type: "Convolution"\u000a  bottom: "conv2/3x3_reduce"\u000a  top: "conv2/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "conv2/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "conv2/3x3"\u000a  top: "conv2/3x3"\u000a}\u000a\u000alayer {\u000a  name: "conv2/norm2"\u000a  type: "LRN"\u000a  bottom: "conv2/3x3"\u000a  top: "conv2/norm2"\u000a  lrn_param {\u000a    local_size: 5\u000a    alpha: 0.0001\u000a    beta: 0.75\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "pool2/3x3_s2"\u000a  type: "Pooling"\u000a  bottom: "conv2/norm2"\u000a  top: "pool2/3x3_s2"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 2\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/1x1"\u000a  type: "Convolution"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/1x1"\u000a  top: "inception_3a/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 96\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/3x3_reduce"\u000a  top: "inception_3a/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/3x3_reduce"\u000a  top: "inception_3a/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/3x3"\u000a  top: "inception_3a/3x3"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 16\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3a/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/5x5_reduce"\u000a  top: "inception_3a/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_3a/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/5x5_reduce"\u000a  top: "inception_3a/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3a/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/5x5"\u000a  top: "inception_3a/5x5"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/pool"\u000a  type: "Pooling"\u000a  bottom: "pool2/3x3_s2"\u000a  top: "inception_3a/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/pool"\u000a  top: "inception_3a/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3a/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_3a/pool_proj"\u000a  top: "inception_3a/pool_proj"\u000a}\u000a\u000alayer {\u000a  name: "inception_3a/output"\u000a  type: "Concat"\u000a  bottom: "inception_3a/1x1"\u000a  bottom: "inception_3a/3x3"\u000a  bottom: "inception_3a/5x5"\u000a  bottom: "inception_3a/pool_proj"\u000a  top: "inception_3a/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/1x1"\u000a  top: "inception_3b/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/3x3_reduce"\u000a  top: "inception_3b/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_3b/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_3b/3x3_reduce"\u000a  top: "inception_3b/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/3x3"\u000a  top: "inception_3b/3x3"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/5x5_reduce"\u000a  top: "inception_3b/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_3b/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_3b/5x5_reduce"\u000a  top: "inception_3b/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 96\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/5x5"\u000a  top: "inception_3b/5x5"\u000a}\u000a\u000alayer {\u000a  name: "inception_3b/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_3a/output"\u000a  top: "inception_3b/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_3b/pool"\u000a  top: "inception_3b/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_3b/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_3b/pool_proj"\u000a  top: "inception_3b/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_3b/output"\u000a  type: "Concat"\u000a  bottom: "inception_3b/1x1"\u000a  bottom: "inception_3b/3x3"\u000a  bottom: "inception_3b/5x5"\u000a  bottom: "inception_3b/pool_proj"\u000a  top: "inception_3b/output"\u000a}\u000a\u000alayer {\u000a  name: "pool3/3x3_s2"\u000a  type: "Pooling"\u000a  bottom: "inception_3b/output"\u000a  top: "pool3/3x3_s2"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 2\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/1x1"\u000a  type: "Convolution"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/1x1"\u000a  top: "inception_4a/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 96\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/3x3_reduce"\u000a  top: "inception_4a/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/3x3_reduce"\u000a  top: "inception_4a/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 208\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/3x3"\u000a  top: "inception_4a/3x3"\u000a}\u000a\u000alayer {\u000a  name: "inception_4a/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 16\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/5x5_reduce"\u000a  top: "inception_4a/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4a/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/5x5_reduce"\u000a  top: "inception_4a/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 48\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/5x5"\u000a  top: "inception_4a/5x5"\u000a}\u000alayer {\u000a  name: "inception_4a/pool"\u000a  type: "Pooling"\u000a  bottom: "pool3/3x3_s2"\u000a  top: "inception_4a/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/pool"\u000a  top: "inception_4a/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4a/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4a/pool_proj"\u000a  top: "inception_4a/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4a/output"\u000a  type: "Concat"\u000a  bottom: "inception_4a/1x1"\u000a  bottom: "inception_4a/3x3"\u000a  bottom: "inception_4a/5x5"\u000a  bottom: "inception_4a/pool_proj"\u000a  top: "inception_4a/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4b/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 160\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4b/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/1x1"\u000a  top: "inception_4b/1x1"\u000a}\u000alayer {\u000a  name: "inception_4b/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 112\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/3x3_reduce"\u000a  top: "inception_4b/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4b/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/3x3_reduce"\u000a  top: "inception_4b/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 224\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/3x3"\u000a  top: "inception_4b/3x3"\u000a}\u000alayer {\u000a  name: "inception_4b/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 24\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/5x5_reduce"\u000a  top: "inception_4b/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4b/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/5x5_reduce"\u000a  top: "inception_4b/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/5x5"\u000a  top: "inception_4b/5x5"\u000a}\u000alayer {\u000a  name: "inception_4b/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4a/output"\u000a  top: "inception_4b/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/pool"\u000a  top: "inception_4b/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4b/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4b/pool_proj"\u000a  top: "inception_4b/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4b/output"\u000a  type: "Concat"\u000a  bottom: "inception_4b/1x1"\u000a  bottom: "inception_4b/3x3"\u000a  bottom: "inception_4b/5x5"\u000a  bottom: "inception_4b/pool_proj"\u000a  top: "inception_4b/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/1x1"\u000a  top: "inception_4c/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000a\u000alayer {\u000a  name: "inception_4c/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/3x3_reduce"\u000a  top: "inception_4c/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4c/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/3x3_reduce"\u000a  top: "inception_4c/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 256\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/3x3"\u000a  top: "inception_4c/3x3"\u000a}\u000alayer {\u000a  name: "inception_4c/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 24\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/5x5_reduce"\u000a  top: "inception_4c/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4c/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/5x5_reduce"\u000a  top: "inception_4c/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/5x5"\u000a  top: "inception_4c/5x5"\u000a}\u000alayer {\u000a  name: "inception_4c/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4b/output"\u000a  top: "inception_4c/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/pool"\u000a  top: "inception_4c/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4c/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4c/pool_proj"\u000a  top: "inception_4c/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4c/output"\u000a  type: "Concat"\u000a  bottom: "inception_4c/1x1"\u000a  bottom: "inception_4c/3x3"\u000a  bottom: "inception_4c/5x5"\u000a  bottom: "inception_4c/pool_proj"\u000a  top: "inception_4c/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4d/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 112\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/1x1"\u000a  top: "inception_4d/1x1"\u000a}\u000alayer {\u000a  name: "inception_4d/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 144\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/3x3_reduce"\u000a  top: "inception_4d/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4d/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/3x3_reduce"\u000a  top: "inception_4d/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 288\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/3x3"\u000a  top: "inception_4d/3x3"\u000a}\u000alayer {\u000a  name: "inception_4d/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/5x5_reduce"\u000a  top: "inception_4d/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4d/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/5x5_reduce"\u000a  top: "inception_4d/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/5x5"\u000a  top: "inception_4d/5x5"\u000a}\u000alayer {\u000a  name: "inception_4d/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4c/output"\u000a  top: "inception_4d/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/pool"\u000a  top: "inception_4d/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 64\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4d/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4d/pool_proj"\u000a  top: "inception_4d/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4d/output"\u000a  type: "Concat"\u000a  bottom: "inception_4d/1x1"\u000a  bottom: "inception_4d/3x3"\u000a  bottom: "inception_4d/5x5"\u000a  bottom: "inception_4d/pool_proj"\u000a  top: "inception_4d/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_4e/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 256\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/1x1"\u000a  top: "inception_4e/1x1"\u000a}\u000alayer {\u000a  name: "inception_4e/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 160\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/3x3_reduce"\u000a  top: "inception_4e/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_4e/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/3x3_reduce"\u000a  top: "inception_4e/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 320\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/3x3"\u000a  top: "inception_4e/3x3"\u000a}\u000alayer {\u000a  name: "inception_4e/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/5x5_reduce"\u000a  top: "inception_4e/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_4e/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/5x5_reduce"\u000a  top: "inception_4e/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/5x5"\u000a  top: "inception_4e/5x5"\u000a}\u000alayer {\u000a  name: "inception_4e/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4d/output"\u000a  top: "inception_4e/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/pool"\u000a  top: "inception_4e/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_4e/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_4e/pool_proj"\u000a  top: "inception_4e/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_4e/output"\u000a  type: "Concat"\u000a  bottom: "inception_4e/1x1"\u000a  bottom: "inception_4e/3x3"\u000a  bottom: "inception_4e/5x5"\u000a  bottom: "inception_4e/pool_proj"\u000a  top: "inception_4e/output"\u000a}\u000a\u000a\u000a\u000alayer {\u000a  name: "inception_5a/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 256\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/1x1"\u000a  top: "inception_5a/1x1"\u000a}\u000a\u000alayer {\u000a  name: "inception_5a/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 160\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.09\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/3x3_reduce"\u000a  top: "inception_5a/3x3_reduce"\u000a}\u000a\u000alayer {\u000a  name: "inception_5a/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/3x3_reduce"\u000a  top: "inception_5a/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 320\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/3x3"\u000a  top: "inception_5a/3x3"\u000a}\u000alayer {\u000a  name: "inception_5a/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 32\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.2\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/5x5_reduce"\u000a  top: "inception_5a/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_5a/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/5x5_reduce"\u000a  top: "inception_5a/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/5x5"\u000a  top: "inception_5a/5x5"\u000a}\u000alayer {\u000a  name: "inception_5a/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_4e/output"\u000a  top: "inception_5a/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/pool"\u000a  top: "inception_5a/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5a/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_5a/pool_proj"\u000a  top: "inception_5a/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_5a/output"\u000a  type: "Concat"\u000a  bottom: "inception_5a/1x1"\u000a  bottom: "inception_5a/3x3"\u000a  bottom: "inception_5a/5x5"\u000a  bottom: "inception_5a/pool_proj"\u000a  top: "inception_5a/output"\u000a}\u000a\u000alayer {\u000a  name: "inception_5b/1x1"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/1x1"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 384\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_1x1"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/1x1"\u000a  top: "inception_5b/1x1"\u000a}\u000alayer {\u000a  name: "inception_5b/3x3_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/3x3_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 192\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_3x3_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/3x3_reduce"\u000a  top: "inception_5b/3x3_reduce"\u000a}\u000alayer {\u000a  name: "inception_5b/3x3"\u000a  type: "Convolution"\u000a  bottom: "inception_5b/3x3_reduce"\u000a  top: "inception_5b/3x3"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 384\u000a    pad: 1\u000a    kernel_size: 3\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_3x3"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/3x3"\u000a  top: "inception_5b/3x3"\u000a}\u000alayer {\u000a  name: "inception_5b/5x5_reduce"\u000a  type: "Convolution"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/5x5_reduce"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 48\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_5x5_reduce"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/5x5_reduce"\u000a  top: "inception_5b/5x5_reduce"\u000a}\u000alayer {\u000a  name: "inception_5b/5x5"\u000a  type: "Convolution"\u000a  bottom: "inception_5b/5x5_reduce"\u000a  top: "inception_5b/5x5"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    pad: 2\u000a    kernel_size: 5\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_5x5"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/5x5"\u000a  top: "inception_5b/5x5"\u000a}\u000alayer {\u000a  name: "inception_5b/pool"\u000a  type: "Pooling"\u000a  bottom: "inception_5a/output"\u000a  top: "inception_5b/pool"\u000a  pooling_param {\u000a    pool: MAX\u000a    kernel_size: 3\u000a    stride: 1\u000a    pad: 1\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/pool_proj"\u000a  type: "Convolution"\u000a  bottom: "inception_5b/pool"\u000a  top: "inception_5b/pool_proj"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 128\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.1\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.2\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "inception_5b/relu_pool_proj"\u000a  type: "ReLU"\u000a  bottom: "inception_5b/pool_proj"\u000a  top: "inception_5b/pool_proj"\u000a}\u000alayer {\u000a  name: "inception_5b/output"\u000a  type: "Concat"\u000a  bottom: "inception_5b/1x1"\u000a  bottom: "inception_5b/3x3"\u000a  bottom: "inception_5b/5x5"\u000a  bottom: "inception_5b/pool_proj"\u000a  top: "inception_5b/output"\u000a}\u000alayer {\u000a  name: "pool5/drop_s1"\u000a  type: "Dropout"\u000a  bottom: "inception_5b/output"\u000a  top: "pool5/drop_s1"\u000a  dropout_param {\u000a    dropout_ratio: 0.4\u000a  }\u000a}\u000alayer {\u000a  name: "cvg/classifier"\u000a  type: "Convolution"\u000a  bottom: "pool5/drop_s1"\u000a  top: "cvg/classifier"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 1\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.\u000a    }\u000a  }\u000a}\u000alayer {\u000a  name: "coverage/sig"\u000a  type: "Sigmoid"\u000a  bottom: "cvg/classifier"\u000a  top: "coverage"\u000a}\u000alayer {\u000a  name: "bbox/regressor"\u000a  type: "Convolution"\u000a  bottom: "pool5/drop_s1"\u000a  top: "bboxes"\u000a  param {\u000a    lr_mult: 1\u000a    decay_mult: 1\u000a  }\u000a  param {\u000a    lr_mult: 2\u000a    decay_mult: 0\u000a  }\u000a  convolution_param {\u000a    num_output: 4\u000a    kernel_size: 1\u000a    weight_filler {\u000a      type: "xavier"\u000a      std: 0.03\u000a    }\u000a    bias_filler {\u000a      type: "constant"\u000a      value: 0.\u000a    }\u000a  }\u000a}\u000a\u000a######################################################################\u000a# End of convolutional network\u000a######################################################################\u000a\u000a# Convert bboxes\u000alayer {\u000a  name: "bbox_mask"\u000a  type: "Eltwise"\u000a  bottom: "bboxes"\u000a  bottom: "coverage-block"\u000a  top: "bboxes-masked"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bbox-norm"\u000a  type: "Eltwise"\u000a  bottom: "bboxes-masked"\u000a  bottom: "size-block"\u000a  top: "bboxes-masked-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "bbox-obj-norm"\u000a  type: "Eltwise"\u000a  bottom: "bboxes-masked-norm"\u000a  bottom: "obj-block"\u000a  top: "bboxes-obj-masked-norm"\u000a  eltwise_param {\u000a    operation: PROD\u000a  }\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000a\u000a# Loss layers\u000alayer {\u000a  name: "bbox_loss"\u000a  type: "L1Loss"\u000a  bottom: "bboxes-obj-masked-norm"\u000a  bottom: "bbox-obj-label-norm"\u000a  top: "loss_bbox"\u000a  loss_weight: 2\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a  name: "coverage_loss"\u000a  type: "EuclideanLoss"\u000a  bottom: "coverage"\u000a  bottom: "coverage-label"\u000a  top: "loss_coverage"\u000a  include { phase: TRAIN }\u000a  include { phase: TEST stage: "val" }\u000a}\u000a\u000a# Cluster bboxes\u000alayer {\u000a    type: 'Python'\u000a    name: 'cluster'\u000a    bottom: 'coverage'\u000a    bottom: 'bboxes'\u000a    top: 'bbox-list'\u000a    python_param {\u000a        module: 'caffe.layers.detectnet.clustering'\u000a        layer: 'ClusterDetections'\u000a        param_str : '1024, 618, 16, 0.6, 3, 0.02, 22, 1'\u000a    }\u000a    include: { phase: TEST }\u000a}\u000a\u000a# Calculate mean average precision\u000alayer {\u000a  type: 'Python'\u000a  name: 'cluster_gt'\u000a  bottom: 'coverage-label'\u000a  bottom: 'bbox-label'\u000a  top: 'bbox-list-label'\u000a  python_param {\u000a      module: 'caffe.layers.detectnet.clustering'\u000a      layer: 'ClusterGroundtruth'\u000a      param_str : '1024, 618, 16, 1'\u000a  }\u000a  include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a    type: 'Python'\u000a    name: 'score'\u000a    bottom: 'bbox-list-label'\u000a    bottom: 'bbox-list'\u000a    top: 'bbox-list-scored'\u000a    python_param {\u000a        module: 'caffe.layers.detectnet.mean_ap'\u000a        layer: 'ScoreDetections'\u000a    }\u000a    include: { phase: TEST stage: "val" }\u000a}\u000alayer {\u000a    type: 'Python'\u000a    name: 'mAP'\u000a    bottom: 'bbox-list-scored'\u000a    top: 'mAP'\u000a    top: 'precision'\u000a    top: 'recall'\u000a    python_param {\u000a        module: 'caffe.layers.detectnet.mean_ap'\u000a        layer: 'mAP'\u000a        param_str : '1024, 618, 16'\u000a    }\u000a    include: { phase: TEST stage: "val" }\u000a}
p253
sS'form.group_name.data'
p254
g204
sS'form.val_interval.data'
p255
F1.0
sS'form.lr_inv_gamma.data'
p256
F0.1
sS'form.lr_poly_power.data'
p257
F3.0
sS'form.snapshot_interval.data'
p258
F1.0
sS'form.lr_step_size.data'
p259
F33.0
sS'form.aug_hsv_use.data'
p260
I00
sS'form.lr_inv_power.data'
p261
F0.5
sS'form.aug_hsv_v.data'
p262
F0.06
sS'form.custom_network_snapshot.data'
p263
V/projeto/download/bvlc_googlenet.caffemodel
p264
sS'form.lr_step_gamma.data'
p265
F0.1
sS'form.lr_multistep_gamma.data'
p266
F0.5
sS'form.select_gpu.data'
p267
Vnext
p268
sS'form.lr_exp_gamma.data'
p269
F0.95
sS'form.use_mean.data'
p270
g112
sS'form.aug_rot.data'
p271
I0
sS'form.train_epochs.data'
p272
I100
sS'form.batch_size.data'
p273
(lp274
I4
asS'form.aug_hsv_h.data'
p275
F0.02
ssg121
F1.0
sS'_id'
p276
S'20170710-221135-1561'
p277
sS'pickver_job_dataset'
p278
I1
sg173
(lp279
((idigits.status
Status
p280
g177
bF1499724695.254347
tp281
a((idigits.status
Status
p282
g180
bF1499724696.600151
tp283
a((idigits.status
Status
p284
g183
bF1499732872.572601
tp285
asb.